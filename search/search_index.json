{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dataframes File Interchange","text":"<p>For data pipelines, it's often advantageous to be able to \"round trip\" a dataframe to disc: the results of what is proccessed at one stage being required by independent stages afterwards.</p> <p>Pandas can store and manipulate various structures on top of a basic table including indexes and dtype specifications. These indexes can be generated from a small number of parameters that cannot naturally be encoded in a column, e.g. a <code>pd.RangeIndex</code>. When saving to a CSV file, these indexes are typically enumerated. Therefore, when reading the dataframe back, it is not actually the same as the original. Although this is handled better when writing to Parquet format, there are still a few issues such as all entries in a column, and so an index, being required to have the same dtype. Similarly, it's often desirable to store some custom structured metadata along with the dataframe, e.g. the author or columnwise metadata such as what unit each column is expressed in.</p> <p><code>df_file_interchange</code> is a wrapper around Pandas that tries to address these requirements. It stores additional metadata in an accompanying YAML file that (mostly) ensures the indexes are recreated properly, column dtypes are specified properly, and that the dataframe read from disc is actually the same as what was written. It also manages the storage of additional (structured) custom metadata.</p> <p>In the future, the intention is also to opportunistically serialise columns when required since there are some dtypes that are not supported by Parquet.</p>"},{"location":"background/","title":"Technical Background","text":""},{"location":"background/#technical-background","title":"Technical Background","text":""},{"location":"background/#storing-index-and-columns-also-an-index-in-the-yaml-file","title":"Storing Index and Columns (also an Index) in the YAML File","text":"<p>This all sounds a bit dubious at first. However, consider that Pandas has several types of <code>Index</code> including <code>Index</code>, <code>RangeIndex</code>, <code>DatetimeIndex</code>, <code>MultiIndex</code>, <code>CategoricalIndex</code>, etc. Some of these, such as <code>Index</code>, represent the index explicitly with list(s) of elements. Others represent the index in a shorthand way, using only a few parameters needed to reconstruct the index, e.g. <code>RangeIndex</code>. The former could fit nicely as an additional column or row in the tabluar data but the latter cannot and is better stored in the YAML file.</p> <p>Ok, so we could, and may eventually, do just that but it adds complexity to the code. Also, the <code>columns</code> in Pandas act as the unique identifier for the columns and, unfortuantely, the columns need not be a simple list of str or ints: it can be a <code>MultiIndex</code> or such, i.e. something that requires deserialization and instantiation (this has to happen before applying dtypes, for example). There are also further complications in how Pandas handles some of this internally in the sense that it's not entirely consistent.</p> <p>This arrangement is not ideal but, for now at least, storing the serialized row index and column index in the YAML file seems a reasonably \"clean\" way to resolve the problem even if this means a much bigger file. We're not storing massive DataFrames so this should be fine since the files are written+read programatically.</p>"},{"location":"code_reference__ci/","title":"Code Reference - Custom Information","text":""},{"location":"code_reference__ci/#df_file_interchangecibase","title":"df_file_interchange.ci.base","text":"<p>The base class for custom info is defined here.</p>"},{"location":"code_reference__ci/#df_file_interchange.ci.base.FIBaseCustomInfo","title":"<code>df_file_interchange.ci.base.FIBaseCustomInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class to store user custom info</p> <p>N.B. This, and any descendent, MUST be able to deserialise based on a provided dictionary!</p> <p>A descendent of this is usually supplied as an object when writing a file to include additional metadata.</p> Source code in <code>df_file_interchange/ci/base.py</code> <pre><code>class FIBaseCustomInfo(BaseModel):\n    \"\"\"Wrapper class to store user custom info\n\n    N.B. This, and any descendent, MUST be able to deserialise based on a\n    provided dictionary!\n\n    A descendent of this is usually supplied as an object when writing a file to\n    include additional metadata.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    unstructured_data: dict = {}\n\n    @classmethod\n    def get_classname(cls) -&gt; str:\n        \"\"\"Returns the classname\n\n        Fairly prosaic but we use this for serialization and deserialization.\n\n        Returns\n        -------\n        str\n            The classname.\n        \"\"\"\n        return cls.__name__\n\n    @computed_field\n    @property\n    def classname(self) -&gt; str:\n        \"\"\"Ensures classname is included in serialization\n\n        By defining this as a computed Pydantic attribute, it's included in any\n        serialization.\n\n        Returns\n        -------\n        str\n            Our classname\n        \"\"\"\n\n        return self.get_classname()\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchange.ci.base.FIBaseCustomInfo.classname","title":"<code>classname: str</code>  <code>property</code>","text":"<p>Ensures classname is included in serialization</p> <p>By defining this as a computed Pydantic attribute, it's included in any serialization.</p> <p>Returns:</p> Type Description <code>str</code> <p>Our classname</p>"},{"location":"code_reference__ci/#df_file_interchange.ci.base.FIBaseCustomInfo.get_classname","title":"<code>get_classname() -&gt; str</code>  <code>classmethod</code>","text":"<p>Returns the classname</p> <p>Fairly prosaic but we use this for serialization and deserialization.</p> <p>Returns:</p> Type Description <code>str</code> <p>The classname.</p> Source code in <code>df_file_interchange/ci/base.py</code> <pre><code>@classmethod\ndef get_classname(cls) -&gt; str:\n    \"\"\"Returns the classname\n\n    Fairly prosaic but we use this for serialization and deserialization.\n\n    Returns\n    -------\n    str\n        The classname.\n    \"\"\"\n    return cls.__name__\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchangecistructured","title":"df_file_interchange.ci.structured","text":"<p>The \"standard\" class for custom info is defined here along with a standard context (mostly to given an example of the format).</p>"},{"location":"code_reference__ci/#df_file_interchange.ci.structured.FIStructuredCustomInfo","title":"<code>df_file_interchange.ci.structured.FIStructuredCustomInfo</code>","text":"<p>               Bases: <code>FIBaseCustomInfo</code></p> <p>Structured custom info</p> <p>This includes extra data (applies to whole of table), and columnwise units.</p> <p>The extra info and column units can, however, be different classes, i.e. the user can inherit from <code>FIBaseExtraInfo</code> or <code>FIBaseUnit</code> to add more fields to extra info or define new units, respectively. This makes matters a little tricky when instantiating from metadata because we have to choose the correct class to create. By default we use the classnames stored in the metainfo and check they're a subclass of the appropriate base class. An explicit context can, however, be passed to <code>.model_validate()</code>.</p> Source code in <code>df_file_interchange/ci/structured.py</code> <pre><code>class FIStructuredCustomInfo(FIBaseCustomInfo):\n    \"\"\"Structured custom info\n\n    This includes extra data (applies to whole of table), and columnwise units.\n\n    The extra info and column units can, however, be different classes, i.e. the\n    user can inherit from `FIBaseExtraInfo` or `FIBaseUnit` to add more fields\n    to extra info or define new units, respectively. This makes matters a little\n    tricky when instantiating from metadata because we have to choose the\n    correct class to create. By default we use the classnames stored in the\n    metainfo and check they're a subclass of the appropriate base class. An\n    explicit context can, however, be passed to `.model_validate()`.\n    \"\"\"\n\n    # Extra meta info that applies to the whole table\n    # Urgh: https://github.com/pydantic/pydantic/issues/7093\n    extra_info: SerializeAsAny[FIBaseExtraInfo]\n\n    # Columnwise unit info\n    col_units: dict[Any, SerializeAsAny[FIBaseUnit]] = {}\n\n    @field_validator(\"extra_info\", mode=\"before\")\n    @classmethod\n    def validator_extra_info(\n        cls, value: dict | FIBaseExtraInfo, info: ValidationInfo\n    ) -&gt; FIBaseExtraInfo:\n        \"\"\"When necessary, instantiate the correct FIBaseExtraInfo from the supplied dict\n\n        This has to happen when supplied a dict because Pydantic needs to know\n        what class to actually instantiate (otherwise it'll instantiate the\n        FIBaseExtraInfo base class).\n\n        A context can be supplied to `.model_validate()`. which then appears as\n        `info.context` here. To see the format, check\n        `generate_default_context()`.\n        \"\"\"\n\n        # Shortcut exit, if we've been passed something with extra_info already\n        # instantiated. We only deal with dicts here.\n        if not isinstance(value, dict):\n            return value\n\n        # Default don't use context\n        clss_extra_info = None\n\n        # Check if we've been supplied a context\n        if info.context and isinstance(info.context, dict):\n            # Get the available classes for extra_info (this should also be a\n            # dictionary)\n            clss_extra_info = info.context.get(\n                \"clss_extra_info\", {\"FIBaseExtraInfo\": FIBaseExtraInfo}\n            )\n            assert isinstance(clss_extra_info, dict)\n\n        # Now process\n        value_classname = value.get(\"classname\", None)\n        if (\n            value_classname\n            and clss_extra_info is not None\n            and value_classname in clss_extra_info.keys()\n        ):\n            # Now instantiate the model\n            extra_info_class = clss_extra_info[value_classname]\n        elif value_classname in globals().keys() and issubclass(\n            globals()[value_classname], FIBaseExtraInfo\n        ):\n            extra_info_class = globals()[value_classname]\n        else:\n            error_msg = f\"Neither context for supplied classname nor is it a subclass of FIBaseExtraInfo. classname={safe_str_output(value_classname)}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        assert issubclass(extra_info_class, FIBaseExtraInfo)\n        return extra_info_class.model_validate(value, context=info.context)\n\n    @field_validator(\"col_units\", mode=\"before\")\n    @classmethod\n    def validator_col_units(\n        cls, value: dict | dict[Any, FIBaseUnit], info: ValidationInfo\n    ) -&gt; dict:\n        \"\"\"When necessary, instantiate the correct units from the supplied dict\n\n        This has to happen when supplied a dict because Pydantic needs to know\n        what class to actually instantiate (otherwise it'll instantiate a dict\n        of FIBaseUnit base class instead of the correct unit class).\n\n        A context can be supplied to `.model_validate()`. which then appears as\n        `info.context` here. To see the format, check\n        `generate_default_context()`.\n        \"\"\"\n\n        # If this happens, we really need to fail.\n        if not isinstance(value, dict):\n            error_msg = f\"col_units should always be a dictionary. Got type={type(value)}, value={safe_str_output(value)}\"\n            logger.error(error_msg)\n            raise TypeError(error_msg)\n\n        # By default we don't use a context\n        clss_col_units = None\n\n        # Check for a context\n        if info.context and isinstance(info.context, dict):\n            # Get the available classes for units (this should also be a\n            # dictionary)\n            clss_col_units = info.context.get(\n                \"clss_col_units\", {\"FIBaseUnit\": FIBaseUnit}\n            )\n            assert isinstance(clss_col_units, dict)\n\n        # Now process each element in value, in turn\n        loc_value = {}\n        for idx in value:\n            # I don't like having to write this explicitly but Pylance\n            # complained on the .get() later if we don't do this (or similar)\n            cur_value = value[idx]\n\n            # Skip if already instantiated\n            if not isinstance(cur_value, dict):\n                loc_value[idx] = value[idx]\n                continue\n\n            value_classname = cur_value.get(\"classname\", None)\n            if (\n                value_classname\n                and clss_col_units is not None\n                and value_classname in clss_col_units.keys()\n            ):\n                # Now instantiate the model and add to our local dictionary\n                units_class = clss_col_units[value_classname]\n            elif value_classname in globals().keys() and issubclass(\n                globals()[value_classname], FIBaseUnit\n            ):\n                units_class = globals()[value_classname]\n            else:\n                error_msg = f\"Neither context supplied nor is subclass of FIBaseUnit. classname={safe_str_output(value_classname)}\"\n                logger.error(error_msg)\n                raise TypeError(error_msg)\n\n            assert issubclass(units_class, FIBaseUnit)\n            loc_value[idx] = units_class.model_validate(\n                value[idx], context=info.context\n            )\n\n        return loc_value\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchange.ci.structured.FIStructuredCustomInfo.validator_col_units","title":"<code>validator_col_units(value: dict | dict[Any, FIBaseUnit], info: ValidationInfo) -&gt; dict</code>  <code>classmethod</code>","text":"<p>When necessary, instantiate the correct units from the supplied dict</p> <p>This has to happen when supplied a dict because Pydantic needs to know what class to actually instantiate (otherwise it'll instantiate a dict of FIBaseUnit base class instead of the correct unit class).</p> <p>A context can be supplied to <code>.model_validate()</code>. which then appears as <code>info.context</code> here. To see the format, check <code>generate_default_context()</code>.</p> Source code in <code>df_file_interchange/ci/structured.py</code> <pre><code>@field_validator(\"col_units\", mode=\"before\")\n@classmethod\ndef validator_col_units(\n    cls, value: dict | dict[Any, FIBaseUnit], info: ValidationInfo\n) -&gt; dict:\n    \"\"\"When necessary, instantiate the correct units from the supplied dict\n\n    This has to happen when supplied a dict because Pydantic needs to know\n    what class to actually instantiate (otherwise it'll instantiate a dict\n    of FIBaseUnit base class instead of the correct unit class).\n\n    A context can be supplied to `.model_validate()`. which then appears as\n    `info.context` here. To see the format, check\n    `generate_default_context()`.\n    \"\"\"\n\n    # If this happens, we really need to fail.\n    if not isinstance(value, dict):\n        error_msg = f\"col_units should always be a dictionary. Got type={type(value)}, value={safe_str_output(value)}\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    # By default we don't use a context\n    clss_col_units = None\n\n    # Check for a context\n    if info.context and isinstance(info.context, dict):\n        # Get the available classes for units (this should also be a\n        # dictionary)\n        clss_col_units = info.context.get(\n            \"clss_col_units\", {\"FIBaseUnit\": FIBaseUnit}\n        )\n        assert isinstance(clss_col_units, dict)\n\n    # Now process each element in value, in turn\n    loc_value = {}\n    for idx in value:\n        # I don't like having to write this explicitly but Pylance\n        # complained on the .get() later if we don't do this (or similar)\n        cur_value = value[idx]\n\n        # Skip if already instantiated\n        if not isinstance(cur_value, dict):\n            loc_value[idx] = value[idx]\n            continue\n\n        value_classname = cur_value.get(\"classname\", None)\n        if (\n            value_classname\n            and clss_col_units is not None\n            and value_classname in clss_col_units.keys()\n        ):\n            # Now instantiate the model and add to our local dictionary\n            units_class = clss_col_units[value_classname]\n        elif value_classname in globals().keys() and issubclass(\n            globals()[value_classname], FIBaseUnit\n        ):\n            units_class = globals()[value_classname]\n        else:\n            error_msg = f\"Neither context supplied nor is subclass of FIBaseUnit. classname={safe_str_output(value_classname)}\"\n            logger.error(error_msg)\n            raise TypeError(error_msg)\n\n        assert issubclass(units_class, FIBaseUnit)\n        loc_value[idx] = units_class.model_validate(\n            value[idx], context=info.context\n        )\n\n    return loc_value\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchange.ci.structured.FIStructuredCustomInfo.validator_extra_info","title":"<code>validator_extra_info(value: dict | FIBaseExtraInfo, info: ValidationInfo) -&gt; FIBaseExtraInfo</code>  <code>classmethod</code>","text":"<p>When necessary, instantiate the correct FIBaseExtraInfo from the supplied dict</p> <p>This has to happen when supplied a dict because Pydantic needs to know what class to actually instantiate (otherwise it'll instantiate the FIBaseExtraInfo base class).</p> <p>A context can be supplied to <code>.model_validate()</code>. which then appears as <code>info.context</code> here. To see the format, check <code>generate_default_context()</code>.</p> Source code in <code>df_file_interchange/ci/structured.py</code> <pre><code>@field_validator(\"extra_info\", mode=\"before\")\n@classmethod\ndef validator_extra_info(\n    cls, value: dict | FIBaseExtraInfo, info: ValidationInfo\n) -&gt; FIBaseExtraInfo:\n    \"\"\"When necessary, instantiate the correct FIBaseExtraInfo from the supplied dict\n\n    This has to happen when supplied a dict because Pydantic needs to know\n    what class to actually instantiate (otherwise it'll instantiate the\n    FIBaseExtraInfo base class).\n\n    A context can be supplied to `.model_validate()`. which then appears as\n    `info.context` here. To see the format, check\n    `generate_default_context()`.\n    \"\"\"\n\n    # Shortcut exit, if we've been passed something with extra_info already\n    # instantiated. We only deal with dicts here.\n    if not isinstance(value, dict):\n        return value\n\n    # Default don't use context\n    clss_extra_info = None\n\n    # Check if we've been supplied a context\n    if info.context and isinstance(info.context, dict):\n        # Get the available classes for extra_info (this should also be a\n        # dictionary)\n        clss_extra_info = info.context.get(\n            \"clss_extra_info\", {\"FIBaseExtraInfo\": FIBaseExtraInfo}\n        )\n        assert isinstance(clss_extra_info, dict)\n\n    # Now process\n    value_classname = value.get(\"classname\", None)\n    if (\n        value_classname\n        and clss_extra_info is not None\n        and value_classname in clss_extra_info.keys()\n    ):\n        # Now instantiate the model\n        extra_info_class = clss_extra_info[value_classname]\n    elif value_classname in globals().keys() and issubclass(\n        globals()[value_classname], FIBaseExtraInfo\n    ):\n        extra_info_class = globals()[value_classname]\n    else:\n        error_msg = f\"Neither context for supplied classname nor is it a subclass of FIBaseExtraInfo. classname={safe_str_output(value_classname)}\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    assert issubclass(extra_info_class, FIBaseExtraInfo)\n    return extra_info_class.model_validate(value, context=info.context)\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchange.ci.structured.generate_default_context","title":"<code>df_file_interchange.ci.structured.generate_default_context()</code>","text":"<p>Generates a default context that is an 'all' for anything that is included by default</p> <p>Returns all available default units and FIStdExtraInfo.</p> <p>If you extend with your own units, you'll have to add manually.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The context.</p> Source code in <code>df_file_interchange/ci/structured.py</code> <pre><code>def generate_default_context():\n    \"\"\"Generates a default context that is an 'all' for anything that is included by default\n\n    Returns all available default units and FIStdExtraInfo.\n\n    If you extend with your own units, you'll have to add manually.\n\n    Returns\n    -------\n    dict\n        The context.\n    \"\"\"\n\n    context = {\n        \"clss_custom_info\": {\n            \"FIStructuredCustomInfo\": FIStructuredCustomInfo,\n        },\n        \"clss_extra_info\": {\n            \"FIStdExtraInfo\": FIStdExtraInfo,\n        },\n        \"clss_col_units\": {\n            \"FICurrencyUnit\": FICurrencyUnit,\n            \"FIPopulationUnit\": FIPopulationUnit,\n        },\n    }\n\n    return context\n</code></pre>"},{"location":"code_reference__ci/#df_file_interchangeciexamples","title":"df_file_interchange.ci.examples","text":"<p>Example(s) of using structured custom info. These are mostly for testing purposes but can be instructive as examples.</p>"},{"location":"code_reference__ci/#df_file_interchange.ci.examples.generate_example_with_metainfo_1","title":"<code>df_file_interchange.ci.examples.generate_example_with_metainfo_1()</code>","text":"<p>Generates a sample dataframe with custom info</p> <p>Returns:</p> Type Description <code>tuple</code> <p>(df, custom_info)</p> Source code in <code>df_file_interchange/ci/examples.py</code> <pre><code>def generate_example_with_metainfo_1():\n    \"\"\"Generates a sample dataframe with custom info\n\n    Returns\n    -------\n    tuple\n        (df, custom_info)\n    \"\"\"\n\n    # Create basic dataframe\n    df = pd.DataFrame(np.random.randn(3, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n    df[\"pop\"] = pd.array([1234, 5678, 91011])\n\n    unit_cur_a = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\n    unit_cur_b = FICurrencyUnit(unit_desc=\"EUR\", unit_multiplier=1000)\n    unit_cur_c = FICurrencyUnit(unit_desc=\"JPY\", unit_multiplier=1000000)\n    unit_cur_d = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\n    unit_pop = FIPopulationUnit(unit_desc=\"people\", unit_multiplier=1)\n\n    extra_info = FIStdExtraInfo(author=\"Spud\", source=\"Potato\")\n\n    custom_info = FIStructuredCustomInfo(\n        extra_info=extra_info,\n        col_units={\n            \"a\": unit_cur_a,\n            \"b\": unit_cur_b,\n            \"c\": unit_cur_c,\n            \"d\": unit_cur_d,\n            \"pop\": unit_pop,\n        },\n    )\n\n    return (df, custom_info)\n</code></pre>"},{"location":"code_reference__ci_extra/","title":"Code Reference - Custom Information - Extra","text":"<p>The definitions for the \"extra\" information classes, i.e. fields that apply to the whole dataframe such as author, etc.</p>"},{"location":"code_reference__ci_extra/#df_file_interchangeciextrabase","title":"df_file_interchange.ci.extra.base","text":""},{"location":"code_reference__ci_extra/#df_file_interchange.ci.extra.base.FIBaseExtraInfo","title":"<code>df_file_interchange.ci.extra.base.FIBaseExtraInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>df_file_interchange/ci/extra/base.py</code> <pre><code>class FIBaseExtraInfo(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def get_classname(cls) -&gt; str:\n        return cls.__name__\n\n    @computed_field\n    @property\n    def classname(self) -&gt; str:\n        \"\"\"Ensures classname is included in serialization\n\n        Returns\n        -------\n        str\n            Our classname\n        \"\"\"\n\n        return self.get_classname()\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def model_validator_extra_info(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"classname\" in data.keys():\n                del data[\"classname\"]\n        return data\n</code></pre>"},{"location":"code_reference__ci_extra/#df_file_interchange.ci.extra.base.FIBaseExtraInfo.classname","title":"<code>classname: str</code>  <code>property</code>","text":"<p>Ensures classname is included in serialization</p> <p>Returns:</p> Type Description <code>str</code> <p>Our classname</p>"},{"location":"code_reference__ci_extra/#df_file_interchangeciextrastd_extra","title":"df_file_interchange.ci.extra.std_extra","text":""},{"location":"code_reference__ci_extra/#df_file_interchange.ci.extra.std_extra.FIStdExtraInfo","title":"<code>df_file_interchange.ci.extra.std_extra.FIStdExtraInfo</code>","text":"<p>               Bases: <code>FIBaseExtraInfo</code></p> <p>Standard extra info for a dataframe</p> <p>Attributes:</p> Name Type Description <code>author</code> <code>str | None</code> <p>Default None.</p> <code>source</code> <code>str | None</code> <p>Where it came from. Default None.</p> Source code in <code>df_file_interchange/ci/extra/std_extra.py</code> <pre><code>class FIStdExtraInfo(FIBaseExtraInfo):\n    \"\"\"Standard extra info for a dataframe\n\n    Attributes\n    ----------\n    author : str | None\n        Default None.\n    source : str | None\n        Where it came from. Default None.\n    \"\"\"\n\n    author: str | None = None\n    source: str | None = None\n    description: str | None = None\n    processed_date: date | datetime | None = None\n    processed_by: str | None = None\n</code></pre>"},{"location":"code_reference__ci_unit/","title":"Code Reference - Custom Information - Units","text":"<p>The definitions for the \"units\" information classes, i.e. columnwise unit definitions.</p>"},{"location":"code_reference__ci_unit/#df_file_interchangeciunitbase","title":"df_file_interchange.ci.unit.base","text":""},{"location":"code_reference__ci_unit/#df_file_interchange.ci.unit.base.FIBaseUnit","title":"<code>df_file_interchange.ci.unit.base.FIBaseUnit</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for units</p> <p>You have to derive from this to define your own unit.</p> <p>Attributes:</p> Name Type Description <code>unit_desc</code> <code>str literal</code> <p>For example, if the unit is a currency then this would be a literal of strs of the possible currencies. See <code>FICurrencyUnit</code> for this example.</p> <code>unit_multiplier</code> <code>float</code> <p>Default 1.0. Used when, for example, we need to say \"millions of (a unit)\".</p> Source code in <code>df_file_interchange/ci/unit/base.py</code> <pre><code>class FIBaseUnit(BaseModel):\n    \"\"\"Base class for units\n\n    You have to derive from this to define your own unit.\n\n    Attributes\n    ----------\n    unit_desc : str literal\n        For example, if the unit is a currency then this would be a literal of\n        strs of the possible currencies. See `FICurrencyUnit` for this example.\n    unit_multiplier : float\n        Default 1.0. Used when, for example, we need to say \"millions of (a\n        unit)\".\n    \"\"\"\n\n    # The unit description , e.g. \"kg\", \"USD\", \"hamsters\"\n    unit_desc: None = None\n\n    # Sometimes we have quantities in \"millions of $\", for example\n    unit_multiplier: int | float = 1.0\n\n    @classmethod\n    def get_classname(cls) -&gt; str:\n        return cls.__name__\n\n    @computed_field\n    @property\n    def classname(self) -&gt; str:\n        \"\"\"Ensures classname is included in serialization\n\n        Returns\n        -------\n        str\n            Our classname\n        \"\"\"\n\n        return self.get_classname()\n</code></pre>"},{"location":"code_reference__ci_unit/#df_file_interchange.ci.unit.base.FIBaseUnit.classname","title":"<code>classname: str</code>  <code>property</code>","text":"<p>Ensures classname is included in serialization</p> <p>Returns:</p> Type Description <code>str</code> <p>Our classname</p>"},{"location":"code_reference__ci_unit/#df_file_interchange.ci.unit.base.FIGenericUnit","title":"<code>df_file_interchange.ci.unit.base.FIGenericUnit</code>","text":"<p>               Bases: <code>FIBaseUnit</code></p> <p>A generic unit, not sure using this now but will keep as placeholder for now...</p> Source code in <code>df_file_interchange/ci/unit/base.py</code> <pre><code>class FIGenericUnit(FIBaseUnit):\n    \"\"\"A generic unit, not sure using this now but will keep as placeholder for now...\"\"\"\n\n    # Override so we can specify arbitrary strings\n    unit_desc: Literal[None] | str = None\n</code></pre>"},{"location":"code_reference__ci_unit/#df_file_interchangeciunitcurrency","title":"df_file_interchange.ci.unit.currency","text":""},{"location":"code_reference__ci_unit/#df_file_interchange.ci.unit.currency.FICurrencyUnit","title":"<code>df_file_interchange.ci.unit.currency.FICurrencyUnit</code>","text":"<p>               Bases: <code>FIBaseUnit</code></p> <p>(Pydantic) class that defines currencies as a unit</p> Notes <p>For defining these (dev), currency codes can be obtained from https://treasury.un.org/operationalrates/OperationalRates.php Download the EXCEL file, copy the column into a text file, then run <code>cat currency_abbreviations.txt | sed 's/ //g;s/^/\"/;s/$/\",/' | sort | uniq  &gt; currency_abbreviations_processed.txt</code> and then you'll need to manually remove the \"USDollar\" entry.</p> <p>Supplemented with Taiwan dollar, \"TWD\"</p> <p>Attributes:</p> Name Type Description <code>unit_desc</code> <code>Literal</code> <p>What currency, e.g. \"USD, \"EUR\", etc.</p> <code>unit_multiplier</code> <code>float</code> <p>How many of <code>unit_desc</code> at one time, e.g. 1_000_000 USD. Default 1.0.</p> <code>unit_year</code> <code>int or None</code> <p>Sometimes we want the currency to be pegged to a specific year, e.g. \"EUR\" in 2004. If using this attribute, must also specify whether its average, year end, etc. Default None.</p> <code>unit_year_method</code> <code>Literal['AVG', 'END'] | None</code> <p>Whether currency has been calculated as an average over a period or is end of period. Default None.</p> <code>unit_date</code> <code>datetime | date | None</code> <p>Sometimes we might want to specify currency against a fixed date. Default None.</p> Source code in <code>df_file_interchange/ci/unit/currency.py</code> <pre><code>class FICurrencyUnit(FIBaseUnit):\n    \"\"\"(Pydantic) class that defines currencies as a unit\n\n    Notes\n    -----\n\n    For defining these (dev), currency codes can be obtained from\n    https://treasury.un.org/operationalrates/OperationalRates.php Download the\n    EXCEL file, copy the column into a text file, then run\n    `cat currency_abbreviations.txt | sed 's/ //g;s/^/\"/;s/$/\",/' | sort | uniq  &gt; currency_abbreviations_processed.txt`\n    and then you'll need to manually remove the \"USDollar\" entry.\n\n    Supplemented with Taiwan dollar, \"TWD\"\n\n    Attributes\n    ----------\n    unit_desc : Literal\n        What currency, e.g. \"USD, \"EUR\", etc.\n\n    unit_multiplier : float\n        How many of `unit_desc` at one time, e.g. 1_000_000 USD. Default 1.0.\n\n    unit_year : int or None\n        Sometimes we want the currency to be pegged to a specific year, e.g.\n        \"EUR\" in 2004. If using this attribute, must also specify whether its\n        average, year end, etc. Default None.\n    unit_year_method: Literal[\"AVG\", \"END\"] | None\n        Whether currency has been calculated as an average over a period or is\n        end of period. Default None.\n\n    unit_date: datetime | date | None\n        Sometimes we might want to specify currency against a fixed date.\n        Default None.\n    \"\"\"\n\n    # The various currencies we can use.\n    unit_desc: Literal[\n        \"AED\",\n        \"AFN\",\n        \"ALL\",\n        \"AMD\",\n        \"ANG\",\n        \"AOA\",\n        \"ARS\",\n        \"AUD\",\n        \"AWG\",\n        \"AZN\",\n        \"BAM\",\n        \"BBD\",\n        \"BDT\",\n        \"BGN\",\n        \"BHD\",\n        \"BIF\",\n        \"BMD\",\n        \"BND\",\n        \"BOB\",\n        \"BRL\",\n        \"BSD\",\n        \"BTN\",\n        \"BWP\",\n        \"BYN\",\n        \"BZD\",\n        \"CAD\",\n        \"CDF\",\n        \"CHF\",\n        \"CLP\",\n        \"CNY\",\n        \"COP\",\n        \"CRC\",\n        \"CUP\",\n        \"CVE\",\n        \"CZK\",\n        \"DJF\",\n        \"DKK\",\n        \"DOP\",\n        \"DZD\",\n        \"EGP\",\n        \"ERN\",\n        \"ETB\",\n        \"EUR\",\n        \"FJD\",\n        \"GBP\",\n        \"GEL\",\n        \"GHS\",\n        \"GIP\",\n        \"GMD\",\n        \"GNF\",\n        \"GTQ\",\n        \"GYD\",\n        \"HKD\",\n        \"HNL\",\n        \"HTG\",\n        \"HUF\",\n        \"IDR\",\n        \"ILS\",\n        \"INR\",\n        \"IQD\",\n        \"IRR\",\n        \"ISK\",\n        \"JMD\",\n        \"JOD\",\n        \"JPY\",\n        \"KES\",\n        \"KGS\",\n        \"KHR\",\n        \"KMF\",\n        \"KPW\",\n        \"KRW\",\n        \"KWD\",\n        \"KYD\",\n        \"KZT\",\n        \"LAK\",\n        \"LBP\",\n        \"LKR\",\n        \"LRD\",\n        \"LSL\",\n        \"LYD\",\n        \"MAD\",\n        \"MDL\",\n        \"MGA\",\n        \"MKD\",\n        \"MMK\",\n        \"MNT\",\n        \"MOP\",\n        \"MRU\",\n        \"MUR\",\n        \"MVR\",\n        \"MWK\",\n        \"MXN\",\n        \"MYR\",\n        \"MZN\",\n        \"NAD\",\n        \"NGN\",\n        \"NIO\",\n        \"NOK\",\n        \"NPR\",\n        \"NZD\",\n        \"OMR\",\n        \"PAB\",\n        \"PEN\",\n        \"PGK\",\n        \"PHP\",\n        \"PKR\",\n        \"PLN\",\n        \"PYG\",\n        \"QAR\",\n        \"RON\",\n        \"RSD\",\n        \"RUB\",\n        \"RWF\",\n        \"SAR\",\n        \"SBD\",\n        \"SCR\",\n        \"SDG\",\n        \"SEK\",\n        \"SGD\",\n        \"SHP\",\n        \"SLE\",\n        \"SOS\",\n        \"SRD\",\n        \"SSP\",\n        \"STN\",\n        \"SYP\",\n        \"SZL\",\n        \"THB\",\n        \"TJS\",\n        \"TMT\",\n        \"TND\",\n        \"TOP\",\n        \"TRY\",\n        \"TTD\",\n        \"TWD\",  # Manually added\n        \"TZS\",\n        \"UAH\",\n        \"UGX\",\n        \"USD\",\n        \"UYU\",\n        \"UZS\",\n        \"VES\",\n        \"VND\",\n        \"VUV\",\n        \"WST\",\n        \"XAF\",\n        \"XCD\",\n        \"XOF\",\n        \"XPF\",\n        \"YER\",\n        \"ZAR\",\n        \"ZMW\",\n        \"ZWL\",\n    ] = \"USD\"\n\n    # Sometimes we have quantities in \"millions of $\", for example\n    unit_multiplier: float = 1.0\n\n    # Sometimes we need currency to be tagged to a specific year, e.g. \"EUR\" in\n    # 2004. If using this field, can also specify whether it's averaged, year\n    # end, etc, in unit_year_method\n    unit_year: int | None = None\n    unit_year_method: Literal[\"AVG\", \"END\"] | None = None\n\n    # Sometimes we might want to specify currency against a fixed date.\n    unit_date: datetime | date | None = None\n\n    @model_validator(mode=\"after\")\n    def model_validator_after(self) -&gt; Self:\n        # 20240522: this doesn't necessarily make sense, so have commented it out...\n        # # Check if unit_year not None then unit_year_method must also be not\n        # # None\n        # if self.unit_year is not None and self.unit_year_method is None:\n        #     error_msg = \"Validator error: if unit_year is not None then unit_year_method must be defined.\"\n        #     logger.error(error_msg)\n        #     raise ValueError(error_msg)\n\n        # Check that if unit_year_method not None then unit_year is not None\n        if self.unit_year_method is not None and self.unit_year is None:\n            error_msg = \"Validation error: if unit_year_method not None then unit_year must also be not None.\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Check unit_year and unit_date aren't both set at the same time\n        if self.unit_year is not None and self.unit_date is not None:\n            error_msg = \"Validation error: unit_year and unit_date cannot both be not None at the same time\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        return self\n</code></pre>"},{"location":"code_reference__ci_unit/#df_file_interchangeciunitpopulation","title":"df_file_interchange.ci.unit.population","text":""},{"location":"code_reference__ci_unit/#df_file_interchange.ci.unit.population.FIPopulationUnit","title":"<code>df_file_interchange.ci.unit.population.FIPopulationUnit</code>","text":"<p>               Bases: <code>FIBaseUnit</code></p> <p>(Pydantic) class that defines population as a unit</p> <p>Includes possibility to define not only as persons but as 'adults', 'children', 'women', 'men', etc.</p> <p>Attributes:</p> Name Type Description <code>unit_desc</code> <code>Literal</code> <p>Whether we're talking about 'people', 'adults', etc.</p> <code>unit_multiplier</code> <code>int</code> <p>How many of <code>unit_desc</code> at one time, e.g. 1_000_000 persons. Default 1.</p> Source code in <code>df_file_interchange/ci/unit/population.py</code> <pre><code>class FIPopulationUnit(FIBaseUnit):\n    \"\"\"(Pydantic) class that defines population as a unit\n\n    Includes possibility to define not only as persons but as 'adults',\n    'children', 'women', 'men', etc.\n\n    Attributes\n    ----------\n    unit_desc : Literal\n        Whether we're talking about 'people', 'adults', etc.\n\n    unit_multiplier : int\n        How many of `unit_desc` at one time, e.g. 1_000_000 persons. Default 1.\n    \"\"\"\n\n    # The various currencies we can use.\n    unit_desc: Literal[\n        \"people\",\n        \"adults\",\n        \"children\",\n        \"pensioners\",\n        \"women\",\n        \"men\",\n    ] = \"people\"\n\n    # This is probably what will be changed rather than unit_desc\n    unit_multiplier: int = 1\n</code></pre>"},{"location":"code_reference__file/","title":"Code Reference","text":""},{"location":"code_reference__file/#df_file_interchangefilerw","title":"df_file_interchange.file.rw","text":"<p>The classes and functions in this module do the writing and reading.</p>"},{"location":"code_reference__file/#premable","title":"Premable","text":""},{"location":"code_reference__file/#df_file_interchange.file.rw.chk_strict_frames_eq_ignore_nan","title":"<code>df_file_interchange.file.rw.chk_strict_frames_eq_ignore_nan(df1: pd.DataFrame, df2: pd.DataFrame)</code>","text":"<p>Check whether two dataframes are equal, ignoring NaNs</p> <p>This may be expensive since we have to make a copy of the dataframes to avoid mangling the originals. Raises exception if dfs are unequal.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>DataFrame</code> required <code>df2</code> <code>DataFrame</code> required <p>Returns:</p> Type Description <code>bool</code> <p>Always True.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def chk_strict_frames_eq_ignore_nan(df1: pd.DataFrame, df2: pd.DataFrame):\n    \"\"\"Check whether two dataframes are equal, ignoring NaNs\n\n    This may be expensive since we have to make a copy of the dataframes to\n    avoid mangling the originals. Raises exception if dfs are unequal.\n\n    Parameters\n    ----------\n    df1 : pd.DataFrame\n    df2 : pd.DataFrame\n\n    Returns\n    -------\n    bool\n        Always True.\n    \"\"\"\n\n    const_float = np.pi\n\n    # Copy the dataframes because we do not want to modify the originals\n    loc_df1 = df1.copy()\n    loc_df2 = df2.copy()\n\n    # Iterate through the columns of each df, and if the column dtype is float then we replace NaNs with a finite value\n    # We don't seem to need to bother to do this for np.complex types, so this seems ok...\n    col_list1 = []\n    col_list2 = []\n    for col in loc_df1:\n        if loc_df1[col].dtype in [\"float16\", \"float32\", \"float64\"]:\n            col_list1.append(col)\n    for col in loc_df2:\n        if loc_df2[col].dtype in [\"float16\", \"float32\", \"float64\"]:\n            col_list2.append(col)\n\n    d_col_list1 = {col: {np.nan: const_float} for col in col_list1}\n    d_col_list2 = {col: {np.nan: const_float} for col in col_list2}\n    loc_df1.replace(to_replace=d_col_list1, inplace=True)\n    loc_df2.replace(to_replace=d_col_list2, inplace=True)\n\n    # Finallly, we can do the test free from NaN != NaN issues.\n    assert_frame_equal(\n        loc_df1,\n        loc_df2,\n        check_dtype=True,\n        check_index_type=True,\n        check_column_type=True,\n        check_categorical=True,\n        check_frame_type=True,\n        check_names=True,\n        check_exact=True,\n        check_freq=True,\n        # check_flag=True,\n    )\n\n    return True\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIFileFormatEnum","title":"<code>df_file_interchange.file.rw.FIFileFormatEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>File formats used by file interchange</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIFileFormatEnum(str, Enum):\n    \"\"\"File formats used by file interchange\"\"\"\n\n    csv = \"csv\"\n    parquet = \"parquet\"\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIndexType","title":"<code>df_file_interchange.file.rw.FIIndexType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The type of an index, e.g. RangeIndex, Categorical, MultiIndex</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIIndexType(str, Enum):\n    \"\"\"The type of an index, e.g. RangeIndex, Categorical, MultiIndex\"\"\"\n\n    base = \"base\"\n    idx = \"idx\"  # Using literal \"index\" seems to cause a problem.\n    range = \"range\"\n    categorical = \"categorical\"\n    multi = \"multi\"\n    interval = \"interval\"\n    datetime = \"datetime\"\n    timedelta = \"timedelta\"\n    period = \"period\"\n</code></pre>"},{"location":"code_reference__file/#encoding-specifications","title":"Encoding Specifications","text":"<p>Encoding options can be specified in a <code>FIEncoding</code> object, which in turn contains <code>FIEncodingCSV</code> and <code>FIEncodingParquet</code> as attributes (only the object corresponding to the file format applies when writing). These all construct themselves with default options, it's usually ill-advised to change these.</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIEncodingCSV","title":"<code>df_file_interchange.file.rw.FIEncodingCSV</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The parameters we use for writing or reading CSV files.</p> <p>NOTE! You almost certainly do not have any reason to change these defaults. They were tested to ensure that the roundtrip write-read is exactly correct.</p> <p>Attributes:</p> Name Type Description <code>csv_allowed_na</code> <code>list[str]</code> <p>Default [\"\"]. WE write all our files, so we can be more restrictive to reduce window for ambiguity when reading a file. In particualr, it's a bad idea to confuse NaN with a null value with a missing value with an empty value -- these are NOT the same, despite what \"data science\" conventions might suggest. If you must be awkward, try [\"-NaN\", \"-nan\", \"\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\", \"n/a\", \"nan\", \"null\"] noting that \"\" is not in that list (that does cause problems). <code>sep</code> <code>str</code> <p>Default \",\". Explictly define field separator</p> <code>na_rep</code> <code>str</code> <p>Default \"\". This must be in the csv_allowed_na list. What's used as the default na. <code>keep_default_na</code> <code>bool</code> <p>Default False.</p> <code>doublequote</code> <code>bool</code> <p>Default True. How we're escaping quotes in a str.</p> <code>quoting</code> <code>int</code> <p>Default csv.QUOTE_NONNUMERIC. i.e. we only quote non-numeric values.</p> <code>float_precision</code> <code>Literal['high', 'legacy', 'round_trip']</code> <p>Default \"round_trip\". Weirdly, Pandas's other options, including the default, don't actually return what was written with floats.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIEncodingCSV(BaseModel):\n    \"\"\"The parameters we use for writing or reading CSV files.\n\n    NOTE! You almost certainly do not have any reason to change these defaults.\n    They were tested to ensure that the roundtrip write-read is exactly correct.\n\n    Attributes\n    ----------\n    csv_allowed_na : list[str]\n        Default [\"&lt;NA&gt;\"]. WE write all our files, so we can be more restrictive\n        to reduce window for ambiguity when reading a file. In particualr, it's\n        a bad idea to confuse NaN with a null value with a missing value with an\n        empty value -- these are NOT the same, despite what \"data science\"\n        conventions might suggest. If you must be awkward, try [\"-NaN\", \"-nan\",\n        \"&lt;NA&gt;\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\", \"n/a\", \"nan\", \"null\"] noting\n        that \"\" is not in that list (that does cause problems).\n\n    sep : str\n        Default \",\". Explictly define field separator\n\n    na_rep: str\n        Default \"&lt;NA&gt;\". This must be in the csv_allowed_na list. What's used as\n        the default na.\n\n    keep_default_na: bool\n        Default False.\n\n    doublequote: bool\n        Default True. How we're escaping quotes in a str.\n\n    quoting: int\n        Default csv.QUOTE_NONNUMERIC. i.e. we only quote non-numeric values.\n\n    float_precision: Literal[\"high\", \"legacy\", \"round_trip\"]\n        Default \"round_trip\". Weirdly, Pandas's other options, including the\n        default, don't actually return what was written with floats.\n\n    \"\"\"\n\n    csv_allowed_na: list[str] = [\"&lt;NA&gt;\"]\n    sep: str = \",\"\n    na_rep: str = \"&lt;NA&gt;\"\n    keep_default_na: bool = False\n    doublequote: bool = True\n    quoting: int = csv.QUOTE_NONNUMERIC\n    float_precision: Literal[\"high\", \"legacy\", \"round_trip\"] = \"round_trip\"\n\n    @model_validator(mode=\"after\")\n    def check_logic(self):\n        if self.na_rep != \"\":\n            if self.na_rep not in self.csv_allowed_na:\n                error_msg = (\n                    f\"na_rep must be in csv_allowed_na. na_rep={safe_str_output(self.na_rep)};\"\n                    f\" csv_allowed_na={safe_str_output(self.csv_allowed_na)}\"\n                )\n                logger.error(error_msg)\n                raise LookupError(error_msg)\n\n        return self\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIEncodingParquet","title":"<code>df_file_interchange.file.rw.FIEncodingParquet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The parameters we used for writing parquet files</p> <p>Again, there's really no need to change these.</p> <p>Attributes:</p> Name Type Description <code>engine</code> <code>str</code> <p>Default \"pyarrow\". Engine to use. Has to be consistent and was tested with pyarrow</p> <code>index</code> <code>str | None</code> <p>Default None. See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIEncodingParquet(BaseModel):\n    \"\"\"The parameters we used for writing parquet files\n\n    Again, there's really no need to change these.\n\n    Attributes\n    ----------\n    engine : str\n        Default \"pyarrow\". Engine to use. Has to be consistent and was tested\n        with pyarrow\n\n    index : str | None\n        Default None. See\n        https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet\n    \"\"\"\n\n    engine: str = \"pyarrow\"\n    index: str | None = None\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIEncoding","title":"<code>df_file_interchange.file.rw.FIEncoding</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>General encoding options, includes CSV and Parquet encoding</p> <p>Attributes:</p> Name Type Description <code>csv</code> <code>FIEncodingCSV</code> <p>Default FIEncodingCSV(). Extra options that depend on format</p> <code>parq</code> <code>FIEncodingParquet</code> <p>Default FIEncodingParquet(). Extra options that depend on format</p> <code>auto_convert_int_to_intna</code> <code>bool</code> <p>Default True. Whether to automatically convert standard int dtypes to Pandas's Int64Dtype (which can also encode NA values), if there are one or more NAs or None(s) in the column</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIEncoding(BaseModel):\n    \"\"\"General encoding options, includes CSV and Parquet encoding\n\n    Attributes\n    ----------\n    csv : FIEncodingCSV\n        Default FIEncodingCSV(). Extra options that depend on format\n\n    parq : FIEncodingParquet\n        Default FIEncodingParquet(). Extra options that depend on format\n\n    auto_convert_int_to_intna : bool\n        Default True. Whether to automatically convert standard int dtypes to\n        Pandas's Int64Dtype (which can also encode NA values), if there are one\n        or more NAs or None(s) in the column\n\n    \"\"\"\n\n    csv: FIEncodingCSV = FIEncodingCSV()\n    parq: FIEncodingParquet = FIEncodingParquet()\n    auto_convert_int_to_intna: bool = True\n</code></pre>"},{"location":"code_reference__file/#our-index-representations","title":"Our Index Representation(s)","text":"<p>We have our own classes to represent Pandas indexes, which can perform operations such as serialization and instantiation (of the Pandas index). Everything here should derive from the <code>FIBaseIndex</code> base class.</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIBaseIndex","title":"<code>df_file_interchange.file.rw.FIBaseIndex</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for our custom classes to be able to serialize/deserialize/instantiate Pandas indexes</p> <p>This is derived from Pydantic <code>BaseModel</code>, so we can (and do) use those facilities.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIBaseIndex(BaseModel):\n    \"\"\"Base class for our custom classes to be able to serialize/deserialize/instantiate Pandas indexes\n\n    This is derived from Pydantic `BaseModel`, so we can (and do) use those\n    facilities.\n    \"\"\"\n\n    # TODO factory code to instantiate itself? (if possible from Pydantic model)\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.base.name\n\n    def get_fi_index_type(self) -&gt; FIIndexType:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.base\n\n    def get_as_index(self, **kwargs) -&gt; pd.Index:\n        \"\"\"Creates corresponding Pandas index\n\n        Params\n        ------\n        **kwargs : dict\n            Not used at current time.\n\n        Returns\n        -------\n        pd.Index\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.Index()\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIBaseIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIBaseIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.Index</code>","text":"<p>Creates corresponding Pandas index</p> Params <p>**kwargs : dict     Not used at current time.</p> <p>Returns:</p> Type Description <code>Index</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.Index:\n    \"\"\"Creates corresponding Pandas index\n\n    Params\n    ------\n    **kwargs : dict\n        Not used at current time.\n\n    Returns\n    -------\n    pd.Index\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.Index()\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIBaseIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; FIIndexType</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; FIIndexType:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.base\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIndex","title":"<code>df_file_interchange.file.rw.FIIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresonds to pd.Index</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.Index.html</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ArrayLike | AnyArrayLike | list | tuple</code> <p>The enumerated elements in the index.</p> <code>name</code> <code>str | None = None</code> <p>Optional name.</p> <code>dtype</code> <code>Dtype | DtypeObj | ExtensionDtype | None</code> <p>Dtype of the elemenets.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIIndex(FIBaseIndex):\n    \"\"\"Corresonds to pd.Index\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.Index.html\n\n    Attributes\n    ----------\n\n    data : ArrayLike | AnyArrayLike | list | tuple\n        The enumerated elements in the index.\n\n    name : str | None = None\n        Optional name.\n\n    dtype : Dtype | DtypeObj | pd.api.extensions.ExtensionDtype | None\n        Dtype of the elemenets.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n    name: str | None = None\n    dtype: Dtype | DtypeObj | pd.api.extensions.ExtensionDtype | None\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.idx.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.idx\n\n    def get_as_index(self, **kwargs) -&gt; pd.Index:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.Index\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n        return pd.Index(\n            data=self.data,\n            name=self.name,\n            dtype=self.dtype,\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(list(data))\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_index_type(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.Index</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>Index</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.Index:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.Index\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n    return pd.Index(\n        data=self.data,\n        name=self.name,\n        dtype=self.dtype,\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.idx\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIRangeIndex","title":"<code>df_file_interchange.file.rw.FIRangeIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresonds to pd.RangeIndex</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.RangeIndex.html</p> <p>Attributes:</p> Name Type Description <code>start</code> <code>int</code> <p>Where index starts counting from.</p> <code>stop</code> <code>int</code> <p>Where index stops counting.</p> <code>step</code> <code>int</code> <p>Step that index counts in.</p> <code>name</code> <code>str | None</code> <p>Optional name. Default None.</p> <code>dtype</code> <code>DtypeObj | ExtensionDtype | str | None</code> <p>Dtype of the index.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIRangeIndex(FIBaseIndex):\n    \"\"\"Corresonds to pd.RangeIndex\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.RangeIndex.html\n\n    Attributes\n    ----------\n\n    start : int\n        Where index starts counting from.\n\n    stop : int\n        Where index stops counting.\n\n    step : int\n        Step that index counts in.\n\n    name : str | None\n        Optional name. Default None.\n\n    dtype : DtypeObj | pd.api.extensions.ExtensionDtype | str | None\n        Dtype of the index.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    start: int\n    stop: int\n    step: int\n    name: str | None = None\n    dtype: DtypeObj | pd.api.extensions.ExtensionDtype | str | None\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.range.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.range\n\n    def get_as_index(self, **kwargs) -&gt; pd.RangeIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.RangeIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.RangeIndex(\n            start=self.start,\n            stop=self.stop,\n            step=self.step,\n            name=self.name,\n            dtype=self.dtype,\n        )\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIRangeIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIRangeIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.RangeIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>RangeIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.RangeIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.RangeIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.RangeIndex(\n        start=self.start,\n        stop=self.stop,\n        step=self.step,\n        name=self.name,\n        dtype=self.dtype,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIRangeIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.range\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FICategoricalIndex","title":"<code>df_file_interchange.file.rw.FICategoricalIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresonds to pd.CategoricalIndex</p> <p>See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.html</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ArrayLike | AnyArrayLike | list | tuple</code> <p>Elements in index.</p> <code>categories</code> <code>ArrayLike | AnyArrayLike | list | tuple</code> <p>List from which elements in data must belong.</p> <code>ordered</code> <code>bool</code> <p>Whether data should be ordered?</p> <code>name</code> <code>str | None</code> <p>Optional name. Default None.</p> <code>dtype</code> <code>DtypeObj | ExtensionDtype | CategoricalDtype | str | None</code> <p>Dtype of elements.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FICategoricalIndex(FIBaseIndex):\n    \"\"\"Corresonds to pd.CategoricalIndex\n\n    See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.CategoricalIndex.html\n\n    Attributes\n    ----------\n\n    data : ArrayLike | AnyArrayLike | list | tuple\n        Elements in index.\n\n    categories : ArrayLike | AnyArrayLike | list | tuple\n        List from which elements in data must belong.\n\n    ordered : bool\n        Whether data should be ordered?\n\n    name : str | None\n        Optional name. Default None.\n\n    dtype : DtypeObj | pd.api.extensions.ExtensionDtype | pd.CategoricalDtype | str | None\n        Dtype of elements.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n    categories: ArrayLike | AnyArrayLike | list | tuple\n    ordered: bool\n    name: str | None = None\n    dtype: (\n        DtypeObj | pd.api.extensions.ExtensionDtype | pd.CategoricalDtype | str | None\n    )\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.categorical.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.categorical\n\n    def get_as_index(self, **kwargs) -&gt; pd.CategoricalIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.CategoricalIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.CategoricalIndex(\n            data=self.data,\n            categories=self.categories,\n            ordered=self.ordered,\n            name=self.name,\n            dtype=self.dtype,\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(list(data))\n\n    @field_serializer(\"categories\", when_used=\"always\")\n    def serialize_categories(self, categories: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(list(categories))\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n            if (\n                \"categories\" in data.keys()\n                and isinstance(data[\"categories\"], dict)\n                and \"el\" in data[\"categories\"].keys()\n                and \"eltype\" in data[\"categories\"].keys()\n            ):\n                data[\"categories\"] = _deserialize_element(data[\"categories\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FICategoricalIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FICategoricalIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.CategoricalIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>CategoricalIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.CategoricalIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.CategoricalIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.CategoricalIndex(\n        data=self.data,\n        categories=self.categories,\n        ordered=self.ordered,\n        name=self.name,\n        dtype=self.dtype,\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FICategoricalIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.categorical\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIMultiIndex","title":"<code>df_file_interchange.file.rw.FIMultiIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresponds to pd.MultiIndex</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.html and https://pandas.pydata.org/docs/user_guide/advanced.html</p> <p>Attributes:</p> Name Type Description <code>levels</code> <code>list</code> <p>The number of levels in the multiindex.</p> <code>codes</code> <code>list</code> <p>The list of lists (I think), of the elements in the index.</p> <code>sortorder</code> <code>int | None</code> <p>Default None.</p> <code>names</code> <code>list</code> <p>List of names for the levels.</p> <code>dtypes</code> <code>Series | list</code> <p>Dtype specifications.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIMultiIndex(FIBaseIndex):\n    \"\"\"Corresponds to pd.MultiIndex\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.html and\n    https://pandas.pydata.org/docs/user_guide/advanced.html\n\n    Attributes\n    ----------\n\n    levels : list\n        The number of levels in the multiindex.\n\n    codes : list\n        The list of lists (I think), of the elements in the index.\n\n    sortorder : int | None\n        Default None.\n\n    names : list\n        List of names for the levels.\n\n    dtypes : pd.Series | list\n        Dtype specifications.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    levels: list\n    codes: list\n    sortorder: int | None = None\n    names: list\n    dtypes: pd.Series | list  # Hmmm.\n\n    # Need some extra validation logic to ensure FrozenList(s) contain what is expected\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.multi.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.multi\n\n    def get_as_index(self, **kwargs) -&gt; pd.MultiIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.MultiIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.MultiIndex(\n            levels=self.levels,\n            codes=self.codes,\n            sortorder=self.sortorder,\n            names=self.names,\n            dtype=self.dtypes,  # Not used in Pandas source\n            copy=True,\n            verify_integrity=True,\n        )\n\n    @field_serializer(\"levels\", when_used=\"always\")\n    def serialize_levels(self, levels: list):\n        loc_levels = []\n        for level in levels:\n            loc_levels.append(_serialize_element(level))\n\n        return loc_levels\n\n    @field_serializer(\"codes\", when_used=\"always\")\n    def serialize_codes(self, codes: list):\n        loc_codes = []\n        for code in codes:\n            loc_codes.append(_serialize_element(code))\n\n        return loc_codes\n\n    @field_serializer(\"names\", when_used=\"always\")\n    def serialize_names(self, names: list):\n        if isinstance(names, np.ndarray):\n            return names.tolist()\n        else:\n            return list(names)\n\n    @field_serializer(\"dtypes\", when_used=\"always\")\n    def serialize_dtypes(self, dtypes: pd.Series | list):\n        # Ouch.\n        return list(map(str, list(dtypes)))\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            # Check if data provided is a \"true\" data array or if it's serialized from before\n            if (\n                \"levels\" in data.keys()\n                and len(data[\"levels\"]) &gt; 0\n                and isinstance(data[\"levels\"], list)\n            ):\n                loc_levels = []\n                for cur_level in data[\"levels\"]:\n                    # Need to test whether we're deserializing or de novo construction\n                    if (\n                        isinstance(cur_level, dict)\n                        and \"el\" in cur_level.keys()\n                        and \"eltype\" in cur_level.keys()\n                    ):\n                        loc_levels.append(_deserialize_element(cur_level))\n                    else:\n                        loc_levels.append(cur_level)\n\n                data[\"levels\"] = loc_levels\n\n            if (\n                \"codes\" in data.keys()\n                and len(data[\"codes\"]) &gt; 0\n                and isinstance(data[\"codes\"], list)\n            ):\n                loc_codes = []\n                for cur_code in data[\"codes\"]:\n                    # Need to test whether we're deserializing or de novo construction\n                    if (\n                        isinstance(cur_code, dict)\n                        and \"el\" in cur_code.keys()\n                        and \"eltype\" in cur_code.keys()\n                    ):\n                        loc_codes.append(_deserialize_element(cur_code))\n                    else:\n                        loc_codes.append(cur_code)\n\n                data[\"codes\"] = loc_codes\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIMultiIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIMultiIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.MultiIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>MultiIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.MultiIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.MultiIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.MultiIndex(\n        levels=self.levels,\n        codes=self.codes,\n        sortorder=self.sortorder,\n        names=self.names,\n        dtype=self.dtypes,  # Not used in Pandas source\n        copy=True,\n        verify_integrity=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIMultiIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.multi\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIntervalIndex","title":"<code>df_file_interchange.file.rw.FIIntervalIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresponds to pd.IntervalIndex</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.IntervalIndex.html</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>IntervalArray | ndarray</code> <p>The data array (of intervals!).</p> <code>closed</code> <code>IntervalClosedType</code> <p>How each interval is closed or not: \"left\", \"right\", \"closed\", \"neither\".</p> <code>name</code> <code>str or None</code> <p>Optional name. Default None.</p> <code>dtype</code> <code>IntervalDtype | str | None</code> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIIntervalIndex(FIBaseIndex):\n    \"\"\"Corresponds to pd.IntervalIndex\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.IntervalIndex.html\n\n    Attributes\n    ----------\n\n    data : pd.arrays.IntervalArray | np.ndarray\n        The data array (of intervals!).\n\n    closed : IntervalClosedType\n        How each interval is closed or not: \"left\", \"right\", \"closed\", \"neither\".\n\n    name : str or None\n        Optional name. Default None.\n\n    dtype : pd.IntervalDtype | str | None\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: pd.arrays.IntervalArray | np.ndarray\n    closed: IntervalClosedType\n    name: str | None = None\n    dtype: pd.IntervalDtype | str | None\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.interval.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.interval\n\n    def get_as_index(self, **kwargs) -&gt; pd.IntervalIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.IntervalIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.IntervalIndex(\n            data=self.data,  # type: ignore\n            closed=self.closed,\n            name=self.name,\n            dtype=self.dtype,  # type: ignore\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: pd.arrays.IntervalArray | np.ndarray):\n        return _serialize_element(list(data))\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n                # Force IntervalArray\n                data[\"data\"] = pd.arrays.IntervalArray(data[\"data\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIntervalIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIntervalIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.IntervalIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>IntervalIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.IntervalIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.IntervalIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.IntervalIndex(\n        data=self.data,  # type: ignore\n        closed=self.closed,\n        name=self.name,\n        dtype=self.dtype,  # type: ignore\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIIntervalIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.interval\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIDatetimeIndex","title":"<code>df_file_interchange.file.rw.FIDatetimeIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresponds to pd.DatetimeIndex</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ArrayLike | AnyArrayLike | list | tuple</code> <p>Array of datetimes.</p> <code>freq</code> <code>_Frequency | None = None</code> <p>Optional frequency. See Pandas docs for what this means.</p> <code>tz</code> <code>tzinfo | str | None</code> <p>Optional tz.</p> <code>name</code> <code>str | None = None</code> <p>Optional name.</p> <code>dtype</code> <code>Dtype | str | None</code> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIDatetimeIndex(FIBaseIndex):\n    \"\"\"Corresponds to pd.DatetimeIndex\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html\n\n    Attributes\n    ----------\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n        Array of datetimes.\n\n    freq: _Frequency | None = None\n        Optional frequency. See Pandas docs for what this means.\n\n    tz: tzinfo | str | None\n        Optional tz.\n\n    name: str | None = None\n        Optional name.\n\n    dtype: Dtype | str | None\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n    freq: _Frequency | None = None\n    tz: tzinfo | str | None  # most what it should be from pandas src\n    name: str | None = None\n    dtype: Dtype | str | None  # Hmmm.\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.datetime.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.datetime\n\n    def get_as_index(self, **kwargs) -&gt; pd.DatetimeIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.DatetimeIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.DatetimeIndex(\n            data=self.data,\n            freq=self.freq,\n            tz=self.tz,\n            name=self.name,\n            dtype=self.dtype,\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(data)\n\n    @field_serializer(\"freq\", when_used=\"always\")\n    def serialize_freq(self, freq):\n        if self.freq is None:\n            return None\n        else:\n            return freq.freqstr\n\n    @field_serializer(\"tz\", when_used=\"always\")\n    def serialize_tz(self, tz):\n        if self.tz is None:\n            return None\n        else:\n            return str(self.tz)\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            # Check if data provided is a \"true\" data array or if it's serialized from before\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIDatetimeIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIDatetimeIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.DatetimeIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>DatetimeIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.DatetimeIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.DatetimeIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.DatetimeIndex(\n        data=self.data,\n        freq=self.freq,\n        tz=self.tz,\n        name=self.name,\n        dtype=self.dtype,\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIDatetimeIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.datetime\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FITimedeltaIndex","title":"<code>df_file_interchange.file.rw.FITimedeltaIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresponds to pd.TimedeltaIndex</p> <p>See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.TimedeltaIndex.html</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>ArrayLike | AnyArrayLike | list | tuple</code> <p>Array of timedeltas.</p> <code>freq</code> <code>str | BaseOffset | None = None</code> <p>Optional frequency. See Pandas docs for details.</p> <code>name</code> <code>str | None = None</code> <p>Optional name.</p> <code>dtype</code> <code>DtypeObj | TimeDelta64DType | Literal['&lt;m8[ns]'] | str | None</code> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FITimedeltaIndex(FIBaseIndex):\n    \"\"\"Corresponds to pd.TimedeltaIndex\n\n    See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.TimedeltaIndex.html\n\n    Attributes\n    ----------\n\n    data : ArrayLike | AnyArrayLike | list | tuple\n        Array of timedeltas.\n\n    freq : str | BaseOffset | None = None\n        Optional frequency. See Pandas docs for details.\n\n    name : str | None = None\n        Optional name.\n\n    dtype : DtypeObj | np.dtypes.TimeDelta64DType | Literal[\"&lt;m8[ns]\"] | str | None\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n    freq: str | BaseOffset | None = None\n    name: str | None = None\n    dtype: (\n        DtypeObj | np.dtypes.TimeDelta64DType | Literal[\"&lt;m8[ns]\"] | str | None\n    )  # Hmmm.\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.timedelta.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.timedelta\n\n    def get_as_index(self, **kwargs) -&gt; pd.TimedeltaIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.TimedeltaIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.TimedeltaIndex(\n            data=self.data,  # type: ignore\n            freq=self.freq,  # type: ignore\n            name=self.name,  # type: ignore\n            dtype=self.dtype,  # type: ignore\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(list(data))\n\n    @field_serializer(\"freq\", when_used=\"always\")\n    def serialize_freq(self, freq):\n        if self.freq is None:\n            return None\n        else:\n            return freq.freqstr\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FITimedeltaIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FITimedeltaIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.TimedeltaIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>TimedeltaIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.TimedeltaIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.TimedeltaIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.TimedeltaIndex(\n        data=self.data,  # type: ignore\n        freq=self.freq,  # type: ignore\n        name=self.name,  # type: ignore\n        dtype=self.dtype,  # type: ignore\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FITimedeltaIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.timedelta\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIPeriodIndex","title":"<code>df_file_interchange.file.rw.FIPeriodIndex</code>","text":"<p>               Bases: <code>FIBaseIndex</code></p> <p>Corresponds to pd.PeriodIndex</p> <p>See https://pandas.pydata.org/docs/reference/api/pandas.PeriodIndex.html</p> <p>data: ArrayLike | AnyArrayLike | list | tuple     Array of periods.</p> <p>freq: _Frequency | None = None     Optional frequency. See Pandas docs.</p> <p>name: str | None = None     Optional name</p> <p>dtype: DtypeObj | pd.PeriodDtype | str | None  # Hmmm.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIPeriodIndex(FIBaseIndex):\n    \"\"\"Corresponds to pd.PeriodIndex\n\n    See https://pandas.pydata.org/docs/reference/api/pandas.PeriodIndex.html\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n        Array of periods.\n\n    freq: _Frequency | None = None\n        Optional frequency. See Pandas docs.\n\n    name: str | None = None\n        Optional name\n\n    dtype: DtypeObj | pd.PeriodDtype | str | None  # Hmmm.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    data: ArrayLike | AnyArrayLike | list | tuple\n    freq: _Frequency | None = None\n    name: str | None = None\n    dtype: DtypeObj | pd.PeriodDtype | str | None  # Hmmm.\n\n    @computed_field(title=\"index_type\")\n    @property\n    def index_type(self) -&gt; str:\n        \"\"\"Get the str name for the index (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.period.name\n\n    def get_fi_index_type(self) -&gt; str:\n        \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n        return FIIndexType.period\n\n    def get_as_index(self, **kwargs) -&gt; pd.PeriodIndex:\n        \"\"\"Creates corresponding Pandas index\n\n        Returns\n        -------\n        pd.PeriodIndex\n            The Pandas index created corresponding to our FIIndex type and data.\n        \"\"\"\n\n        return pd.PeriodIndex(\n            data=self.data,\n            # freq=self.freq,   -- disabled because it seems to mess things up, info included in dtype and freq is old notation (QE-DEC instead of Q-DEC)\n            name=self.name,\n            dtype=self.dtype,\n            copy=True,\n        )\n\n    @field_serializer(\"data\", when_used=\"always\")\n    def serialize_data(self, data: ArrayLike | AnyArrayLike | list | tuple):\n        return _serialize_element(data)\n\n    @field_serializer(\"freq\", when_used=\"always\")\n    def serialize_freq(self, freq):\n        if self.freq is None:\n            return None\n        else:\n            return freq.freqstr\n\n    @field_serializer(\"dtype\", when_used=\"always\")\n    def serialize_dtype(self, dtype: Dtype | None):\n        return str(dtype)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if (\n                \"data\" in data.keys()\n                and isinstance(data[\"data\"], dict)\n                and \"el\" in data[\"data\"].keys()\n                and \"eltype\" in data[\"data\"].keys()\n            ):\n                data[\"data\"] = _deserialize_element(data[\"data\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIPeriodIndex.index_type","title":"<code>index_type: str</code>  <code>property</code>","text":"<p>Get the str name for the index (one of the FIIndex enum entires)</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIPeriodIndex.get_as_index","title":"<code>get_as_index(**kwargs) -&gt; pd.PeriodIndex</code>","text":"<p>Creates corresponding Pandas index</p> <p>Returns:</p> Type Description <code>PeriodIndex</code> <p>The Pandas index created corresponding to our FIIndex type and data.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_as_index(self, **kwargs) -&gt; pd.PeriodIndex:\n    \"\"\"Creates corresponding Pandas index\n\n    Returns\n    -------\n    pd.PeriodIndex\n        The Pandas index created corresponding to our FIIndex type and data.\n    \"\"\"\n\n    return pd.PeriodIndex(\n        data=self.data,\n        # freq=self.freq,   -- disabled because it seems to mess things up, info included in dtype and freq is old notation (QE-DEC instead of Q-DEC)\n        name=self.name,\n        dtype=self.dtype,\n        copy=True,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIPeriodIndex.get_fi_index_type","title":"<code>get_fi_index_type() -&gt; str</code>","text":"<p>Get the index type (one of the FIIndex enum entires)</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def get_fi_index_type(self) -&gt; str:\n    \"\"\"Get the index type (one of the FIIndex enum entires)\"\"\"\n\n    return FIIndexType.period\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.FIMetainfo","title":"<code>df_file_interchange.file.rw.FIMetainfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>All the collected metadata we use when saving or loading</p> <p>N.B. The order of the attributes is important in the sense that the serialization automatically preserves the order, and then <code>yaml.dump()</code> does too. This means we can make the YAML file a little easier to read/parse by a human.</p> <p>Attributes:</p> Name Type Description <code>datafile</code> <code>Path</code> <p>Ironically, this should always just be the filename with no paths</p> <code>file_format</code> <code>FIFileFormatEnum</code> <p>The file format of datafile.</p> <code>format_version</code> <code>int</code> <p>Default 1. Not really used yet but we might need to version the YAML file.</p> <code>hash</code> <code>str | None</code> <p>SHA256 hash of the datafile.</p> <code>encoding</code> <code>FIEncoding</code> <p>How the datafile was or is to be encoded.</p> <code>custom_info</code> <code>SerializeAsAny[FIBaseCustomInfo]</code> <p>Structured custom info. Can just be an empty FIBaseCustomInfo object.</p> <code>serialized_dtypes</code> <code>dict</code> <p>Dtypes of the dataframe.</p> <code>index</code> <code>FIBaseIndex</code> <p>Index information encoded as a FIBaseIndex object (descendent thereof).</p> <code>columns</code> <code>FIBaseIndex</code> <p>Columns, again, specified as an FIIndex object</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>class FIMetainfo(BaseModel):\n    \"\"\"All the collected metadata we use when saving or loading\n\n    N.B. The _order_ of the attributes is important in the sense that the\n    serialization automatically preserves the order, and then `yaml.dump()` does\n    too. This means we can make the YAML file a little easier to read/parse by a\n    human.\n\n    Attributes\n    ----------\n\n    datafile : Path\n        Ironically, this should always just be the filename with no paths\n\n    file_format : FIFileFormatEnum\n        The file format of datafile.\n\n    format_version: int\n        Default 1. Not really used yet but we might need to version the YAML file.\n\n    hash: str | None\n        SHA256 hash of the datafile.\n\n    encoding: FIEncoding\n        How the datafile was or is to be encoded.\n\n    custom_info: SerializeAsAny[FIBaseCustomInfo]\n        Structured custom info. Can just be an empty FIBaseCustomInfo object.\n\n    serialized_dtypes: dict\n        Dtypes of the dataframe.\n\n    index: FIBaseIndex\n        Index information encoded as a FIBaseIndex object (descendent thereof).\n\n    columns: FIBaseIndex\n        Columns, again, specified as an FIIndex object\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    # Ironically, this should always just be the filename with no paths\n    datafile: Path\n\n    # File format\n    file_format: FIFileFormatEnum\n\n    # Format version\n    format_version: int = 1\n\n    # SHA256 hash\n    hash: str | None = None\n\n    # Encoding\n    encoding: FIEncoding\n\n    # Custom info (user defined metainfo)\n    custom_info: SerializeAsAny[FIBaseCustomInfo]\n\n    # Serialized dtypes\n    serialized_dtypes: dict\n\n    # Index information encoded as a FIIndex object\n    index: FIBaseIndex\n\n    # Columns, again, as an FIIndex object\n    columns: FIBaseIndex\n\n    @field_serializer(\"datafile\", when_used=\"always\")\n    def serialize_datafile(self, datafile: Path):\n        return str(datafile)\n\n    @field_serializer(\"file_format\", when_used=\"always\")\n    def serialize_file_format(self, file_format: FIFileFormatEnum):\n        return file_format.name\n\n    @field_serializer(\"index\", when_used=\"always\")\n    def serialize_index(self, index: FIBaseIndex):\n        # TODO is this ok if caller does a model_dump_json()?\n        return index.model_dump()\n\n    @field_serializer(\"columns\", when_used=\"always\")\n    def serialize_columns(self, columns: FIBaseIndex):\n        # TODO is this ok if caller does a model_dump_json()?\n        return columns.model_dump()\n\n    @field_validator(\"custom_info\", mode=\"before\")\n    @classmethod\n    def validator_custom_info(\n        cls, value: dict | FIBaseCustomInfo, info: ValidationInfo\n    ) -&gt; FIBaseCustomInfo:\n        # Shortcut exit, if we've been passed something with extra_info already\n        # instantiated. We only deal with dicts here.\n        if not isinstance(value, dict):\n            return value\n\n        # By default we don't use a context\n        clss_custom_info = None\n\n        # If we don't have context, just use the base class or return as-is\n        if info.context and isinstance(info.context, dict):\n            # Get the available classes for extra_info (this should also be a\n            # dictionary)\n            clss_custom_info = info.context.get(\n                \"clss_custom_info\", {\"FIBaseCustomInfo\": FIBaseCustomInfo}\n            )\n            assert isinstance(clss_custom_info, dict)\n\n        # Now process\n        value_classname = value.get(\"classname\", None)\n        if (\n            value_classname\n            and clss_custom_info is not None\n            and value_classname in clss_custom_info.keys()\n        ):\n            # Now instantiate the model\n            custom_info_class = clss_custom_info[value_classname]\n        elif value_classname in globals().keys() and issubclass(\n            globals()[value_classname], FIBaseCustomInfo\n        ):\n            custom_info_class = globals()[value_classname]\n        else:\n            error_msg = f\"Neither context for supplied classname nor is it a subclass of FIBaseCustomInfo. classname={safe_str_output(value_classname)}\"\n            logger.error(error_msg)\n            raise TypeError(error_msg)\n\n        assert issubclass(custom_info_class, FIBaseCustomInfo)\n        return custom_info_class.model_validate(value, context=info.context)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_process(cls, data: Any) -&gt; Any:\n        # TODO perhaps move index and columns into separate field validators.\n\n        if isinstance(data, dict):\n            # Need to ensure the index and columns and custominfo are created as\n            # the correct object type, not just instantiating the base class.\n            if \"index\" in data.keys() and isinstance(data[\"index\"], dict):\n                data[\"index\"] = _deserialize_index_dict_to_fi_index(data[\"index\"])\n\n            if \"columns\" in data.keys() and isinstance(data[\"columns\"], dict):\n                data[\"columns\"] = _deserialize_index_dict_to_fi_index(data[\"columns\"])\n\n        return data\n</code></pre>"},{"location":"code_reference__file/#the-write-and-read-functions","title":"The Write and Read Functions","text":"<p>These are what are exposed to the user, to roundtrip write and read dataframes.</p>"},{"location":"code_reference__file/#df_file_interchange.file.rw.write_df_to_file","title":"<code>df_file_interchange.file.rw.write_df_to_file(df: pd.DataFrame, datafile: Path | str, metafile: Path | str | None = None, file_format: FIFileFormatEnum | Literal['csv', 'parquet'] | None = None, encoding: FIEncoding | None = None, custom_info: FIBaseCustomInfo | dict = {}, preprocess_inplace=True) -&gt; Path</code>","text":"<p>Writes a dataframe to file</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to save.</p> required <code>datafile</code> <code>Path or str</code> <p>The datafile to save the dataframe to.</p> required <code>metafile</code> <code>Path or str or None(optional)</code> <p>Metafile name, can be only the filename or with a path (which must be the same as for datafile). If not supplied or None, will be determined automatically.</p> <code>None</code> <code>file_format</code> <code>FIFileFormatEnum | Literal['csv', 'parquet'] | None</code> <p>The file format. If not supplied will be determined automatically.</p> <code>None</code> <code>encoding</code> <code>FIEncoding | None</code> <p>Datafile encoding options.</p> <code>None</code> <code>custom_info</code> <code>FIBaseCustomInfo or dict</code> <p>Custom user metadata to be stored. IF supplied as a FIBaseCustomInfo (or descendent) then it stores things properly. If supplied as a dict, then will create a FIBaseCustomInfo class and store the dictionary in the <code>unstructured_data</code> field.</p> <code>{}</code> <code>preprocess_inplace</code> <code>bool</code> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object with the metainfo filename in it.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def write_df_to_file(\n    df: pd.DataFrame,\n    datafile: Path | str,\n    metafile: Path | str | None = None,\n    file_format: FIFileFormatEnum | Literal[\"csv\", \"parquet\"] | None = None,\n    encoding: FIEncoding | None = None,\n    custom_info: FIBaseCustomInfo | dict = {},\n    preprocess_inplace=True,\n) -&gt; Path:\n    \"\"\"Writes a dataframe to file\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe to save.\n    datafile : Path or str\n        The datafile to save the dataframe to.\n    metafile : Path or str or None (optional)\n        Metafile name, can be only the filename or with a path (which must be\n        the same as for datafile). If not supplied or None, will be determined\n        automatically.\n    file_format : FIFileFormatEnum | Literal['csv', 'parquet'] | None\n        The file format. If not supplied will be determined automatically.\n    encoding : FIEncoding | None, optional\n        Datafile encoding options.\n    custom_info : FIBaseCustomInfo or dict\n        Custom user metadata to be stored. IF supplied as a FIBaseCustomInfo (or\n        descendent) then it stores things properly. If supplied as a dict, then\n        will create a FIBaseCustomInfo class and store the dictionary in the\n        `unstructured_data` field.\n    preprocess_inplace : bool, optional\n\n    Returns\n    -------\n    Path\n        A Path object with the metainfo filename in it.\n\n    \"\"\"\n\n    # Check types and existence correct for datafile and metafile\n    if not isinstance(datafile, (Path, str)):\n        error_msg = f\"datafile must be a Path or str. Got type={type(datafile)}, value={safe_str_output(datafile)}\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    if metafile is not None and not isinstance(metafile, (Path, str)):\n        error_msg = \"When metafile given (not None), it must be a Path or str.\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    # Cast datafile and metafile to str\n    if isinstance(datafile, str):\n        datafile = Path(datafile)\n\n    if isinstance(metafile, str):\n        metafile = Path(metafile)\n\n    # Determine output format\n    if file_format is None:\n        loc_file_format = _detect_file_format_from_filename(datafile)\n    else:\n        loc_file_format = FIFileFormatEnum(file_format)\n\n    # Determine metafile name\n    loc_metafile = _check_metafile_name(datafile, metafile)\n\n    # If we've got encoding parameters, use them; otherwise use defaults\n    if encoding is None:\n        encoding = FIEncoding()\n\n    # Preprocess\n    if preprocess_inplace:\n        _preprocess_inplace(df, encoding)\n        loc_df = df\n    else:\n        loc_df = _preprocess_safe(df, encoding)\n\n    # Deal with custom info situation\n    if isinstance(custom_info, dict):\n        # We create FIBaseCustomInfo ourselves, and assign the dictionary into\n        # unstructured_data\n        loc_custom_info = FIBaseCustomInfo.model_validate(\n            {\"unstructured_data\": custom_info}\n        )\n    elif isinstance(custom_info, FIBaseCustomInfo):\n        loc_custom_info = custom_info\n    else:\n        raise TypeError(\"custom_info must be a dict or descendent of FIBaseCustomInfo\")\n\n    # Write to the data file\n    if loc_file_format == FIFileFormatEnum.csv:\n        _write_to_csv(loc_df, datafile, encoding)\n    elif loc_file_format == FIFileFormatEnum.parquet:\n        _write_to_parquet(loc_df, datafile, encoding)\n    else:\n        error_msg = \"Output format not supported. This shouldn't happen.\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    # Calculate the file's hash\n    with open(datafile, \"rb\") as h_datafile:\n        digest = hashlib.file_digest(h_datafile, \"sha256\")\n    hash = digest.hexdigest()\n\n    # Compile all the metainfo into a dictionary\n    metainfo = _compile_metainfo(\n        datafile=datafile,\n        file_format=loc_file_format,\n        hash=hash,\n        encoding=encoding,\n        custom_info=loc_custom_info,\n        df=loc_df,\n    )\n\n    # Write metafile\n    _write_metafile(datafile, loc_metafile, metainfo)\n\n    return loc_metafile\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.write_df_to_csv","title":"<code>df_file_interchange.file.rw.write_df_to_csv(df: pd.DataFrame, datafile: Path | str, encoding: FIEncoding | None = None, custom_info: FIBaseCustomInfo | dict = {}, preprocess_inplace=True) -&gt; Path</code>","text":"<p>Simplified wrapper around <code>write_df_to_file()</code> to write dataframe to CSV</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe.</p> required <code>datafile</code> <code>Path or str</code> <p>Target datafile.</p> required <code>encoding</code> <code>FIEncoding | None</code> <p>Encoding specs, can be left None for defaults.</p> <code>None</code> <code>custom_info</code> <code>dict</code> <p>Any custom meta data.</p> <code>{}</code> <code>preprocess_inplace</code> <code>bool</code> <p>Whether to do preprocessing inplace (might modify original), by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object with the metainfo filename in it.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def write_df_to_csv(\n    df: pd.DataFrame,\n    datafile: Path | str,\n    encoding: FIEncoding | None = None,\n    custom_info: FIBaseCustomInfo | dict = {},\n    preprocess_inplace=True,\n) -&gt; Path:\n    \"\"\"Simplified wrapper around `write_df_to_file()` to write dataframe to CSV\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe.\n    datafile : Path or str\n        Target datafile.\n    encoding : FIEncoding | None, optional\n        Encoding specs, can be left None for defaults.\n    custom_info : dict, optional\n        Any custom meta data.\n    preprocess_inplace : bool, optional\n        Whether to do preprocessing inplace (might modify original), by default True\n\n    Returns\n    -------\n    Path\n        A Path object with the metainfo filename in it.\n    \"\"\"\n\n    return write_df_to_file(\n        df=df,\n        datafile=datafile,\n        metafile=None,\n        file_format=FIFileFormatEnum.csv,\n        encoding=encoding,\n        custom_info=custom_info,\n        preprocess_inplace=preprocess_inplace,\n    )\n</code></pre>"},{"location":"code_reference__file/#df_file_interchange.file.rw.read_df","title":"<code>df_file_interchange.file.rw.read_df(metafile: Path | str, strict_hash_check: bool = True, context_metainfo: dict | None = None) -&gt; tuple[pd.DataFrame, FIMetainfo]</code>","text":"<p>Load a dataframe from file</p> <p>Supply the metainfo filename, not the datafilename.</p> <p>Parameters:</p> Name Type Description Default <code>metafile</code> <code>Path</code> <p>The YAML file that is associated with the datafile.</p> required <code>strict_hash_check</code> <code>bool</code> <p>Whether we raise an exception if the hash is wrong.</p> <code>True</code> <code>context_metainfo</code> <code>dict | None</code> <p>If manually supplying a context to decode the structured custom info, by default None (in which was subclass type checks are used).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[pd.DataFrame, FIMetainfo]:</code> <p>A tuple with the dataframe and the metainfo object.</p> Source code in <code>df_file_interchange/file/rw.py</code> <pre><code>def read_df(\n    metafile: Path | str,\n    strict_hash_check: bool = True,\n    context_metainfo: dict | None = None,\n) -&gt; tuple[pd.DataFrame, FIMetainfo]:\n    \"\"\"Load a dataframe from file\n\n    Supply the metainfo filename, not the datafilename.\n\n    Parameters\n    ----------\n    metafile : Path\n        The YAML file that is associated with the datafile.\n    strict_hash_check : bool, optional\n        Whether we raise an exception if the hash is wrong.\n    context_metainfo : dict | None, optional\n        If manually supplying a context to decode the structured custom info, by\n        default None (in which was subclass type checks are used).\n\n    Returns\n    -------\n    tuple[pd.DataFrame, FIMetainfo]:\n        A tuple with the dataframe and the metainfo object.\n    \"\"\"\n\n    # Check metafile not empty and correct types\n    if not isinstance(metafile, (Path, str)):\n        error_msg = f\"metafile must be a Path or str. Got type={type(metafile)}, value={safe_str_output(metafile)}\"\n        logger.error(error_msg)\n        raise TypeError(error_msg)\n\n    if isinstance(metafile, str):\n        metafile = Path(metafile)\n\n    # Load metainfo\n    metainfo = _read_metafile(metafile, context=context_metainfo)\n\n    # Check datafile's hash\n    datafile_abs = Path(metafile.parent / metainfo.datafile).resolve()\n    with open(datafile_abs, \"rb\") as h_datafile:\n        digest = hashlib.file_digest(h_datafile, \"sha256\")\n    hash = digest.hexdigest()\n    if hash != metainfo.hash:\n        error_msg = f\"Hash comparison failed. metainfo.hash={safe_str_output(metainfo.hash)}, calcualted hash={safe_str_output(hash)}.\"\n        if strict_hash_check:\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        else:\n            logger.warning(error_msg)\n\n    # Need to know number of columns\n    if isinstance(metainfo.index, FIMultiIndex):\n        num_index_cols = len(metainfo.index.levels)\n    else:\n        num_index_cols = 1\n\n    if isinstance(metainfo.columns, FIMultiIndex):\n        num_index_rows = len(metainfo.columns.levels)\n    else:\n        num_index_rows = 1\n\n    # Load the data\n    if metainfo.file_format == FIFileFormatEnum.csv:\n        df = _read_from_csv(\n            datafile_abs,\n            metainfo.encoding,\n            dtypes=metainfo.serialized_dtypes,\n            num_index_cols=num_index_cols,\n            num_index_rows=num_index_rows,\n        )\n    elif metainfo.file_format == FIFileFormatEnum.parquet:\n        df = _read_from_parquet(datafile_abs, metainfo.encoding)\n    else:\n        error_msg = f\"Input format ({safe_str_output(metainfo.file_format)}) not supported. We only support CSV and Parquet.\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    # Apply index and columns\n    df.index = metainfo.index.get_as_index()\n    df.columns = metainfo.columns.get_as_index()\n\n    # Apply dtypes\n    _apply_serialized_dtypes(df, metainfo.serialized_dtypes)\n\n    return (df, metainfo)\n</code></pre>"},{"location":"custom_info/","title":"Using Custom Structured Info","text":"<p>There's support for storing both unstructured and custom structured (verifiable) metadata in the metafile. This is under <code>df_file_interchange.ci</code> (\"custom info\"). </p> <p>Strictly speaking, the user can pass any object that inherits from <code>FIBaseCustomInfo</code> (a Pydantic model), which can fully serialize itself. There is, however, a standard set of these classes already defined to cover most use-cases.</p> <p>The  <code>df_file_interchange.ci.structured.FIStructuredCustomInfo</code> class is the canonical way to proceed. It has two attributes: <code>extra_info: FIBaseExtraInto</code> and <code>col_units: dict[Any, FIBaseUnit]</code>. The <code>extra_info</code> is for stroing metadata that applies to the whole dataframe such as author or its source. The <code>col_units</code> is a dictionary that specifies the unit, such as currency, columnwise. There are useful classes for both of these under <code>df_file_interchange.ci.extra</code> and <code>df_file_interchange.ci.unit</code>, respectively.</p>"},{"location":"custom_info/#basic-example","title":"Basic Example","text":"<p>A basic example is included here and also in the corresponding notebook on using custom info.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport df_file_interchange as fi\nfrom pathlib import Path\n\nfrom df_file_interchange.ci.extra.std_extra import FIStdExtraInfo\nfrom df_file_interchange.ci.structured import FIStructuredCustomInfo\nfrom df_file_interchange.ci.unit.currency import FICurrencyUnit\nfrom df_file_interchange.ci.unit.population import FIPopulationUnit\n</code></pre> <p>Next, we create an example dataframe. We also an example extra_info object, some units, and put those together as custom info.</p> <pre><code># Create basic dataframe\ndf = pd.DataFrame(np.random.randn(3, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\ndf[\"pop\"] = pd.array([1234, 5678, 91011])\n\n# Define some units\nunit_cur_a = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\nunit_cur_b = FICurrencyUnit(unit_desc=\"EUR\", unit_multiplier=1000)\nunit_cur_c = FICurrencyUnit(unit_desc=\"JPY\", unit_multiplier=1000000)\nunit_cur_d = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\nunit_pop = FIPopulationUnit(unit_desc=\"people\", unit_multiplier=1)\n\n# Define some extra info\nextra_info = FIStdExtraInfo(author=\"Spud\", source=\"Potato\")\n\n# Put that together into a custom_info object\ncustom_info = FIStructuredCustomInfo(\n    extra_info=extra_info,\n    col_units={\n        \"a\": unit_cur_a,\n        \"b\": unit_cur_b,\n        \"c\": unit_cur_c,\n        \"d\": unit_cur_d,\n        \"pop\": unit_pop,\n    },\n)\n</code></pre> <p>We write it to a file.</p> <pre><code>data_dir = Path(\"./data/\")\ndata_dir.mkdir(exist_ok=True)\ndatafile_csv = Path(data_dir / \"tutorial_simple_structured_custom_info.csv\")\n\nmetafile = fi.write_df_to_file(df, datafile_csv, custom_info=custom_info)\n</code></pre> <p>Read it in again.</p> <pre><code>(df_reload, metainfo_reload) = fi.read_df(metafile)\n</code></pre> <p>And, we can inspect the custom info we have read back in.</p> <pre><code>metainfo_reload.custom_info\n</code></pre> <pre><code>FIStructuredCustomInfo(unstructured_data={}, extra_info=FIStdExtraInfo(author='Spud', source='Potato', classname='FIStdExtraInfo'), col_units={'a': FICurrencyUnit(unit_desc='USD', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'b': FICurrencyUnit(unit_desc='EUR', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'c': FICurrencyUnit(unit_desc='JPY', unit_multiplier=1000000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'd': FICurrencyUnit(unit_desc='USD', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'pop': FIPopulationUnit(unit_desc='people', unit_multiplier=1, classname='FIPopulationUnit')}, classname='FIStructuredCustomInfo')\n</code></pre> <p>(not very readable)</p> <p>We'll do a string dump (serialization) of the custom info:</p> <pre><code>metainfo_reload.custom_info.model_dump()\n</code></pre> <pre><code>{'unstructured_data': {},\n 'extra_info': {'author': 'Spud',\n  'source': 'Potato',\n  'classname': 'FIStdExtraInfo'},\n 'col_units': {'a': {'unit_desc': 'USD',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'b': {'unit_desc': 'EUR',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'c': {'unit_desc': 'JPY',\n   'unit_multiplier': 1000000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'd': {'unit_desc': 'USD',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'pop': {'unit_desc': 'people',\n   'unit_multiplier': 1,\n   'classname': 'FIPopulationUnit'}},\n 'classname': 'FIStructuredCustomInfo'}\n</code></pre>"},{"location":"custom_info/#extending-with-your-own-classes","title":"Extending with Your Own Classes","text":"<p>It's easy to extend the provided custom info classes. The important thing is to include a context when reading the dataframe again, so that df_file_interchange knows which classes to instantiate.</p> <p>Here, we use an example from the tests.</p> <pre><code>class FITestUnit(FIBaseUnit):\n    unit_desc: str = \"\"\n    hamster_type: Literal[\"syrian\", \"space-hamster\"] = \"syrian\"\n\n\nclass FITestExtraInfo(FIStdExtraInfo):\n    hamster_collective: str | None = None\n\n\nclass FITestStructuredCustomInfo(FIStructuredCustomInfo):\n    hamster_motto: str | None = None\n\n    df = pd.DataFrame(\n        np.random.randn(4, 3),\n        columns=[\"pet hamsters\", \"wild hamsters\", \"space hamsters\"],\n    )\n\n    unit_cur_pet = FITestUnit(unit_desc=\"hamster\", hamster_type=\"syrian\")\n    unit_cur_wild = FITestUnit(unit_desc=\"hamster\", hamster_type=\"syrian\")\n    unit_cur_space = FITestUnit(unit_desc=\"hamster\", hamster_type=\"space-hamster\")\n\n    extra_info = FITestExtraInfo(\n        author=\"Spud\", source=\"Potato\", hamster_collective=\"going nuts\"\n    )\n\n    custom_info = FITestStructuredCustomInfo(\n        extra_info=extra_info,\n        col_units={\n            \"unit_cur_pet\": unit_cur_pet,\n            \"unit_cur_wild\": unit_cur_wild,\n            \"unit_cur_space\": unit_cur_space,\n        },\n        hamster_motto=\"jump, baby\",\n    )\n\n    # Save CSV\n    target_datafile_csv = tmp_path / \"test_df_extend_ci_1__csv.csv\"\n    metafile = fi.write_df_to_csv(\n        df,\n        target_datafile_csv,\n        custom_info=custom_info,\n    )\n\n    # Define necessary context\n    context = {\n        \"clss_custom_info\": {\n            \"FITestStructuredCustomInfo\": FITestStructuredCustomInfo,\n        },\n        \"clss_extra_info\": {\n            \"FITestExtraInfo\": FITestExtraInfo,\n        },\n        \"clss_col_units\": {\n            \"FITestUnit\": FITestUnit,\n        },\n    }\n\n    # Read\n    (df_reload, metainfo_reload) = fi.read_df(metafile, context_metainfo=context)\n\n    # Compare dataframes\n    chk_strict_frames_eq_ignore_nan(\n        df,\n        df_reload,\n    )\n\n    # Check classes are correct\n    assert isinstance(metainfo_reload.custom_info, FITestStructuredCustomInfo)\n    assert isinstance(metainfo_reload.custom_info.extra_info, FITestExtraInfo)\n    assert isinstance(metainfo_reload.custom_info.col_units[\"unit_cur_pet\"], FITestUnit)\n    assert isinstance(\n        metainfo_reload.custom_info.col_units[\"unit_cur_wild\"], FITestUnit\n    )\n    assert isinstance(\n        metainfo_reload.custom_info.col_units[\"unit_cur_space\"], FITestUnit\n    )\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Install from PyPi or conda-forge.</p>"},{"location":"getting_started/#simple-write-and-read","title":"Simple Write and Read","text":"<p>Ok, lets first import NumPy, Pandas, pathlib, and df_file_interchange.</p> <p>There's a copy of this tutorial in the handy notebook on simple write and write.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport df_file_interchange as fi\nfrom pathlib import Path\n</code></pre> <p>Create a simple dataframe.</p> <pre><code>df = pd.DataFrame(\n    {\n        \"a\": [1, 2, 3, 4, 5],\n        \"b\": [\"apples\", \"pears\", \"oranges\", \"bananas\", \"bears\"],\n        \"c\": [np.pi, 2*np.pi, 3*np.pi, 4*np.pi, 5*np.pi],\n        \"d\": [\n            np.datetime64(\"2010-01-31T10:23:01\"),\n            np.datetime64(\"2014-01-01T10:23:01\"),\n            np.datetime64(\"2018-02-28T10:23:01\"),\n            np.datetime64(\"2024-01-31T10:23:01\"),\n            np.datetime64(\"1999-01-31T23:59:59\")]\n    },\n    index=pd.RangeIndex(start=10, stop=15, step=1),\n)\n</code></pre> <p>Now, lets save the dataframe as a CSV. Minimally, only the dataframe and the filename of the data file to write to is required. <code>df_file_interchange</code> can determine the file format from the extension of the supplied data filename.</p> <pre><code>data_dir = Path(\"./data/\")\ndata_dir.mkdir(exist_ok=True)\ndatafile = Path(data_dir / \"tutorial_trying_out_a_save.csv\")\n\nmetafile = fi.write_df_to_file(df, datafile)\n</code></pre> <p>This will create two files: <code>tutorial_trying_out_a_save.csv</code> and <code>tutorial_trying_out_a_save.yaml</code>. The YAML file describes how the CSV file was encoded, the indexes, the dtypes, ancillary data like the hash of the data file, and (more generally) any custom metadata provided.</p> <p>The <code>metafile</code> variable returned is a <code>Path</code> object pointing to the location of the YAML file.</p> <p>Lets read that file back in.</p> <p><pre><code>(df_reload, metainfo_reload) = fi.read_df(metafile)\n</code></pre> The <code>df_reload</code> variable is the dataframe as read in from the file. The <code>metainfo_reload</code> is a <code>FIMetaInfo</code> object.</p> <p>We can confirm that <code>df_reload</code> is the same as the original <code>df</code> by running</p> <pre><code>fi.chk_strict_frames_eq_ignore_nan(df, df_reload)\n</code></pre> <p>This is just a wrapper around <code>pd._testing.assert_frame_equal()</code> where we instead assume <code>NaN == NaN</code> (unlike the standards definition that <code>NaN != NaN</code>).</p>"},{"location":"getting_started/#variations-on-a-theme","title":"Variations on a Theme","text":"<p>For writing a df to file, there are convenience functions that specify the output type (<code>fi.write_df_to_parquet()</code> and <code>fi.write_df_to_csv()</code>).</p> <pre><code>datafile_parq = Path(data_dir / \"tutorial_trying_out_a_save.parq\")\nfi.write_df_to_parquet(df, datafile_parq)\n</code></pre> <p>Additional parameters can be used, e.g. <code>metafile</code> to specify the YAML file manually (must be same dir) and <code>custom_info</code>.</p> <pre><code>metafile = fi.write_df_to_file(\n    df, datafile, metafile, \"csv\"\n)\n</code></pre> <p>Additional encoding options can be specified using the <code>encoding</code> argument (as a <code>FIEncoding</code> object) but this is unnecessary and probably unwise.</p>"},{"location":"yaml_fields/","title":"YAML Fields","text":"<p>The YAML file used to store the metainfo contains a number of fields. The data for some of these is serialized using our custom serializer <code>file.rw._serialize_element()</code> and <code>file.rw._deserialize_element()</code>.</p> <p>N.B. We store the hash of the datafile in the metainfo, so if you modify the datafile then you also must adjust the hash accordingly. It's an interchange format, so it's assumed users are NOT manually editing stuff.</p>"},{"location":"yaml_fields/#general-fields","title":"General Fields","text":"<p><code>datafile</code>: The filename of the datafile being described.</p> <p><code>encoding</code>: How the datafile is encoded, is a serialized (just a standard serialization) of an <code>FIEncoding</code> object.</p> <p><code>file_format</code>: Whether datafile is csv or parquet.</p> <p><code>format_version</code>: We might need to use some sort of versioning for the file format.</p> <p><code>hash</code>: SHA256 hash of the datafile.</p>"},{"location":"yaml_fields/#indexes","title":"Index(es)","text":"<p>Both the row and column indexes are encoded in a similar way. Advice: don't mess about trying to edit this manually.</p> <p><code>columns</code>: Serialized representation of the column index.</p> <p><code>index</code>: Serialized representation of the row index.</p>"},{"location":"yaml_fields/#dtypes","title":"Dtypes","text":"<p>The column dtypes are specified in <code>serialized_dtypes</code>, again in a specific format. Advice: don't try to mess about with this manually.</p>"},{"location":"yaml_fields/#custom-info","title":"Custom Info","text":"<p>Serialized from the structured custom info. Again, in a specific format. Note that the classnames of the relevant Pydantic objects are included here, so that they can be instantiated properly upon reading.</p>"},{"location":"yaml_fields/#example","title":"Example","text":"<pre><code># Metadata for test_with_structured_custom_info.csv\n---\n\ncolumns:\n  data:\n    el:\n    - el: a\n      eltype: str\n    - el: b\n      eltype: str\n    - el: c\n      eltype: str\n    - el: d\n      eltype: str\n    - el: pop\n      eltype: str\n    eltype: list\n  dtype: object\n  index_type: idx\n  name: null\ncustom_info:\n  classname: FIStructuredCustomInfo\n  col_units:\n    a:\n      classname: FICurrencyUnit\n      unit_date: null\n      unit_desc: USD\n      unit_multiplier: 1000.0\n      unit_year: null\n      unit_year_method: null\n    b:\n      classname: FICurrencyUnit\n      unit_date: null\n      unit_desc: EUR\n      unit_multiplier: 1000.0\n      unit_year: null\n      unit_year_method: null\n    c:\n      classname: FICurrencyUnit\n      unit_date: null\n      unit_desc: JPY\n      unit_multiplier: 1000000.0\n      unit_year: null\n      unit_year_method: null\n    d:\n      classname: FICurrencyUnit\n      unit_date: null\n      unit_desc: USD\n      unit_multiplier: 1000.0\n      unit_year: null\n      unit_year_method: null\n    pop:\n      classname: FIPopulationUnit\n      unit_desc: people\n      unit_multiplier: 1\n  extra_info:\n    author: Spud\n    classname: FIStdExtraInfo\n    source: A simple test\n  unstructured_data: {}\ndatafile: test_with_structured_custom_info.csv\nencoding:\n  auto_convert_int_to_intna: true\n  csv:\n    csv_allowed_na:\n    - &lt;NA&gt;\n    doublequote: true\n    float_precision: round_trip\n    keep_default_na: false\n    na_rep: &lt;NA&gt;\n    quoting: 2\n    sep: ','\n  parq:\n    engine: pyarrow\n    index: null\nfile_format: csv\nformat_version: 1\nhash: ac3f2e11e0ff9ba540b630f03d32652560ef6a224778184b30271ce542b4a59a\nindex:\n  dtype: int64\n  index_type: range\n  name: null\n  start: 0\n  step: 1\n  stop: 3\nserialized_dtypes:\n  a:\n    dtype_str: float64\n    serialized_col_name:\n      el: a\n      eltype: str\n  b:\n    dtype_str: float64\n    serialized_col_name:\n      el: b\n      eltype: str\n  c:\n    dtype_str: float64\n    serialized_col_name:\n      el: c\n      eltype: str\n  d:\n    dtype_str: float64\n    serialized_col_name:\n      el: d\n      eltype: str\n  pop:\n    dtype_str: Int64\n    serialized_col_name:\n      el: pop\n      eltype: str\n</code></pre>"},{"location":"notebooks/tutorial_simple_structured_custom_info/","title":"Notebook Simple Custom Info","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport df_file_interchange as fi\nfrom pathlib import Path\n\n# Make things a little easier in terms of syntax\nfrom df_file_interchange.ci.extra.std_extra import FIStdExtraInfo\nfrom df_file_interchange.ci.structured import FIStructuredCustomInfo\nfrom df_file_interchange.ci.unit.currency import FICurrencyUnit\nfrom df_file_interchange.ci.unit.population import FIPopulationUnit\n</code></pre> <pre><code># Create basic dataframe\ndf = pd.DataFrame(np.random.randn(3, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\ndf[\"pop\"] = pd.array([1234, 5678, 91011])\n\n# Define some units\nunit_cur_a = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\nunit_cur_b = FICurrencyUnit(unit_desc=\"EUR\", unit_multiplier=1000)\nunit_cur_c = FICurrencyUnit(unit_desc=\"JPY\", unit_multiplier=1000000)\nunit_cur_d = FICurrencyUnit(unit_desc=\"USD\", unit_multiplier=1000)\nunit_pop = FIPopulationUnit(unit_desc=\"people\", unit_multiplier=1)\n\n# Define some extra info\nextra_info = FIStdExtraInfo(author=\"Spud\", source=\"Potato\")\n\n# Put that together into a custom_info object\ncustom_info = FIStructuredCustomInfo(\n    extra_info=extra_info,\n    col_units={\n        \"a\": unit_cur_a,\n        \"b\": unit_cur_b,\n        \"c\": unit_cur_c,\n        \"d\": unit_cur_d,\n        \"pop\": unit_pop,\n    },\n)\n</code></pre> <pre><code># Now, lets write the dataframe to file\n\ndata_dir = Path(\"./data/\")\ndata_dir.mkdir(exist_ok=True)\ndatafile_csv = Path(data_dir / \"tutorial_simple_structured_custom_info.csv\")\n\n# Write to a CSV file (file format determined by extension of datafile_csv_path)\nmetafile_yaml = fi.write_df_to_file(df, datafile_csv, custom_info=custom_info)\n</code></pre> <pre><code>metafile_yaml\n</code></pre> <pre>\n<code>PosixPath('data/tutorial_simple_structured_custom_info.yaml')</code>\n</pre> <pre><code># Read the dataframe back into df_reload along with metainfo in metainfo_reload\n(df_reload, metainfo_reload) = fi.read_df(metafile_yaml)\n</code></pre> <pre><code># Check df_reload is the same as the original (only for demo purposes)\nfi.chk_strict_frames_eq_ignore_nan(df, df_reload)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># Lets have a look at the custom info we read from the file\nmetainfo_reload.custom_info\n</code></pre> <pre>\n<code>FIStructuredCustomInfo(unstructured_data={}, extra_info=FIStdExtraInfo(author='Spud', source='Potato', description=None, processed_date=None, processed_by=None, classname='FIStdExtraInfo'), col_units={'a': FICurrencyUnit(unit_desc='USD', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'b': FICurrencyUnit(unit_desc='EUR', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'c': FICurrencyUnit(unit_desc='JPY', unit_multiplier=1000000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'd': FICurrencyUnit(unit_desc='USD', unit_multiplier=1000.0, unit_year=None, unit_year_method=None, unit_date=None, classname='FICurrencyUnit'), 'pop': FIPopulationUnit(unit_desc='people', unit_multiplier=1, classname='FIPopulationUnit')}, classname='FIStructuredCustomInfo')</code>\n</pre> <pre><code># The custom info must always be able to serialize itself, so we can dump that\nmetainfo_reload.custom_info.model_dump()\n</code></pre> <pre>\n<code>{'unstructured_data': {},\n 'extra_info': {'author': 'Spud',\n  'source': 'Potato',\n  'description': None,\n  'processed_date': None,\n  'processed_by': None,\n  'classname': 'FIStdExtraInfo'},\n 'col_units': {'a': {'unit_desc': 'USD',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'b': {'unit_desc': 'EUR',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'c': {'unit_desc': 'JPY',\n   'unit_multiplier': 1000000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'd': {'unit_desc': 'USD',\n   'unit_multiplier': 1000.0,\n   'unit_year': None,\n   'unit_year_method': None,\n   'unit_date': None,\n   'classname': 'FICurrencyUnit'},\n  'pop': {'unit_desc': 'people',\n   'unit_multiplier': 1,\n   'classname': 'FIPopulationUnit'}},\n 'classname': 'FIStructuredCustomInfo'}</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"notebooks/tutorial_simple_write_read/","title":"Notebook Simple Write and Read","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport df_file_interchange as fi\nfrom pathlib import Path\n</code></pre> <pre><code># Create a simple dataframe\n\ndf = pd.DataFrame(\n    {\n        \"a\": [1, 2, 3, 4, 5],\n        \"b\": [\"apples\", \"pears\", \"oranges\", \"bananas\", \"bears\"],\n        \"c\": [np.pi, 2*np.pi, 3*np.pi, 4*np.pi, 5*np.pi],\n        \"d\": [\n            np.datetime64(\"2010-01-31T10:23:01\"),\n            np.datetime64(\"2014-01-01T10:23:01\"),\n            np.datetime64(\"2018-02-28T10:23:01\"),\n            np.datetime64(\"2024-01-31T10:23:01\"),\n            np.datetime64(\"1999-01-31T23:59:59\")]\n    },\n    index=pd.RangeIndex(start=10, stop=15, step=1),\n)\n</code></pre> <pre><code>df\n</code></pre> a b c d 10 1 apples 3.141593 2010-01-31 10:23:01 11 2 pears 6.283185 2014-01-01 10:23:01 12 3 oranges 9.424778 2018-02-28 10:23:01 13 4 bananas 12.566371 2024-01-31 10:23:01 14 5 bears 15.707963 1999-01-31 23:59:59 <pre><code>data_dir = Path(\"./data/\")\ndata_dir.mkdir(exist_ok=True)\ndatafile_csv_path = Path(data_dir / \"tutorial_trying_out_a_save.csv\")\n\n# Write to a CSV file (file format determined by extension of datafile_csv_path)\nmetafile = fi.write_df_to_file(df, datafile_csv_path)\n</code></pre> <pre><code>metafile\n</code></pre> <pre>\n<code>PosixPath('data/tutorial_trying_out_a_save.yaml')</code>\n</pre> <pre><code># Read the dataframe back into df_reload along with metainfo in metainfo_reload\n(df_reload, metainfo_reload) = fi.read_df(metafile)\n</code></pre> <pre><code>df_reload\n</code></pre> a b c d 10 1 apples 3.141593 2010-01-31 10:23:01 11 2 pears 6.283185 2014-01-01 10:23:01 12 3 oranges 9.424778 2018-02-28 10:23:01 13 4 bananas 12.566371 2024-01-31 10:23:01 14 5 bears 15.707963 1999-01-31 23:59:59 <pre><code># The metainfo is supplied as a FIMetaInfo object, which contains as its\n# attributes other objects.\nmetainfo_reload\n</code></pre> <pre>\n<code>FIMetainfo(datafile=PosixPath('tutorial_trying_out_a_save.csv'), file_format=&lt;FIFileFormatEnum.csv: 'csv'&gt;, format_version=1, hash='980eae93340cbcef0d111da0b439a5f8b58f64cf6ab6f923ecb3ce0e0da84e18', encoding=FIEncoding(csv=FIEncodingCSV(csv_allowed_na=['&lt;NA&gt;'], sep=',', na_rep='&lt;NA&gt;', keep_default_na=False, doublequote=True, quoting=2, float_precision='round_trip'), parq=FIEncodingParquet(engine='pyarrow', index=None), auto_convert_int_to_intna=True), custom_info=FIBaseCustomInfo(unstructured_data={}, classname='FIBaseCustomInfo'), serialized_dtypes={'a': {'dtype_str': 'int64', 'serialized_col_name': {'el': 'a', 'eltype': 'str'}}, 'b': {'dtype_str': 'object', 'serialized_col_name': {'el': 'b', 'eltype': 'str'}}, 'c': {'dtype_str': 'float64', 'serialized_col_name': {'el': 'c', 'eltype': 'str'}}, 'd': {'dtype_str': 'datetime64[ns]', 'serialized_col_name': {'el': 'd', 'eltype': 'str'}}}, index=FIRangeIndex(start=10, stop=15, step=1, name=None, dtype='int64', index_type='range'), columns=FIIndex(data=['a', 'b', 'c', 'd'], name=None, dtype='object', index_type='idx'))</code>\n</pre> <pre><code># Lets have a quick look at what the YAML file contains (we'll come back to this\n# in a different tutorial)\nwith open(metafile, 'r') as h_file:\n    print(h_file.read())\n</code></pre> <pre>\n<code># Metadata for &lt;function safe_str_output at 0x7f5d696004a0&gt;\n---\n\ncolumns:\n  data:\n    el:\n    - el: a\n      eltype: str\n    - el: b\n      eltype: str\n    - el: c\n      eltype: str\n    - el: d\n      eltype: str\n    eltype: list\n  dtype: object\n  index_type: idx\n  name: null\ncustom_info:\n  classname: FIBaseCustomInfo\n  unstructured_data: {}\ndatafile: tutorial_trying_out_a_save.csv\nencoding:\n  auto_convert_int_to_intna: true\n  csv:\n    csv_allowed_na:\n    - &lt;NA&gt;\n    doublequote: true\n    float_precision: round_trip\n    keep_default_na: false\n    na_rep: &lt;NA&gt;\n    quoting: 2\n    sep: ','\n  parq:\n    engine: pyarrow\n    index: null\nfile_format: csv\nformat_version: 1\nhash: 980eae93340cbcef0d111da0b439a5f8b58f64cf6ab6f923ecb3ce0e0da84e18\nindex:\n  dtype: int64\n  index_type: range\n  name: null\n  start: 10\n  step: 1\n  stop: 15\nserialized_dtypes:\n  a:\n    dtype_str: int64\n    serialized_col_name:\n      el: a\n      eltype: str\n  b:\n    dtype_str: object\n    serialized_col_name:\n      el: b\n      eltype: str\n  c:\n    dtype_str: float64\n    serialized_col_name:\n      el: c\n      eltype: str\n  d:\n    dtype_str: datetime64[ns]\n    serialized_col_name:\n      el: d\n      eltype: str\n\n</code>\n</pre> <pre><code># Now we check the original dataframe, df, and df_reload read from disc are the\n# same. We use chk_strict_frames_eq_ignore_nan() because, in this context, we\n# want NaN == NaN (usually NaN != NaN)\nfi.chk_strict_frames_eq_ignore_nan(df, df_reload)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code># There are convenience functions to write CSV or Parquet explicitly\ndatafile_parq_path = Path(data_dir / \"./tutorial_trying_out_a_save.parq\")\nfi.write_df_to_parquet(df, datafile_parq_path)\n</code></pre> <pre>\n<code>PosixPath('data/tutorial_trying_out_a_save.yaml')</code>\n</pre> <pre><code># The file for the metainfo can be specified for the write but it must be in the\n# same directory as teh data file. The output format can also be specified\n# explicitly.\nfi.write_df_to_file(df, datafile_csv_path, Path(data_dir / \"tutorial_trying_out_a_save_diff_metafile.yaml\"), file_format=\"csv\")\n</code></pre> <pre>\n<code>PosixPath('data/tutorial_trying_out_a_save_diff_metafile.yaml')</code>\n</pre> <pre><code># Additional encoding options can be supplied but this is almost never a good\n# idea (the defaults were carefully chosen)\nencoding_csv = fi.file.rw.FIEncodingCSV(sep=\";\")\nencoding = fi.file.rw.FIEncoding(csv=encoding_csv)\nmetafile_new_sep = fi.write_df_to_file(df, Path(data_dir / \"tutorial_tring_out_a_save_new_sep.csv\"), encoding=encoding)\n(df_new_sep, metainfo_new_sep) = fi.read_df(metafile_new_sep)\nfi.chk_strict_frames_eq_ignore_nan(df, df_new_sep)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"}]}